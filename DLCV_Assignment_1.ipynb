{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86b6c6b17f3f47ab899f4777d785abe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6539af09cb564c07802c5cb79661d693",
              "IPY_MODEL_9b15ca9083e44b938b9700632f9ce31d",
              "IPY_MODEL_5234d5e7c20e4b868849d28dcd1e7512"
            ],
            "layout": "IPY_MODEL_321f87bb4d254d468abdc1cab1af5c0c"
          }
        },
        "6539af09cb564c07802c5cb79661d693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e0960322cf64fe1a12a251394b6b486",
            "placeholder": "​",
            "style": "IPY_MODEL_4e973c47fce24a25acd6bb3229c4d1b4",
            "value": "100%"
          }
        },
        "9b15ca9083e44b938b9700632f9ce31d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f910419696e9444ebdd2a9d05f37794a",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80839d032c064836a2373d6afe9d5012",
            "value": 46830571
          }
        },
        "5234d5e7c20e4b868849d28dcd1e7512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48d20ee3cd904f81b64dcd796298b638",
            "placeholder": "​",
            "style": "IPY_MODEL_1be4dc216f28483d86bb4d6c1b508e31",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 198MB/s]"
          }
        },
        "321f87bb4d254d468abdc1cab1af5c0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e0960322cf64fe1a12a251394b6b486": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e973c47fce24a25acd6bb3229c4d1b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f910419696e9444ebdd2a9d05f37794a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80839d032c064836a2373d6afe9d5012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48d20ee3cd904f81b64dcd796298b638": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be4dc216f28483d86bb4d6c1b508e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Extra Libraries**"
      ],
      "metadata": {
        "id": "hmZzY7RGLXig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyfiglet\n",
        "%pip install tqdm"
      ],
      "metadata": {
        "id": "ivyubcb3n00G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48218c17-7433-43f4-826a-f9235924b4e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyfiglet\n",
            "  Downloading pyfiglet-0.8.post1-py2.py3-none-any.whl (865 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.8/865.8 KB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyfiglet\n",
            "Successfully installed pyfiglet-0.8.post1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "KFqt6N_TLgOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from os.path import exists\n",
        "import numpy as np\n",
        "import sys\n",
        "import math\n",
        "import cv2\n",
        "import urllib\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "\n",
        "\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import use as mpl_use\n",
        "from pyfiglet import Figlet\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "print('Import Complete')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tjJXDEILfia",
        "outputId": "9a5a26de-953e-4f35-ff7b-102f1ea52a67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Extractor**"
      ],
      "metadata": {
        "id": "KX6DQYxQPPXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_name_to_module(model):\n",
        "    name_to_module = {}\n",
        "    for m in model.named_modules():\n",
        "        name_to_module[m[0]] = m[1]\n",
        "    return name_to_module\n",
        "\n",
        "\n",
        "def get_activation(all_outputs, name):\n",
        "    def hook(model, input, output):\n",
        "        all_outputs[name] = output.detach()\n",
        "\n",
        "    return hook\n",
        "\n",
        "\n",
        "def add_hooks(model, outputs, output_layer_names):\n",
        "    \"\"\"\n",
        "    :param model:\n",
        "    :param outputs: Outputs from layers specified in `output_layer_names` will be stored in `output` variable\n",
        "    :param output_layer_names:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    name_to_module = get_name_to_module(model)\n",
        "    for output_layer_name in output_layer_names:\n",
        "        name_to_module[output_layer_name].register_forward_hook(get_activation(outputs, output_layer_name))\n",
        "\n",
        "\n",
        "class ModelWrapper(nn.Module):\n",
        "    def __init__(self, model, output_layer_names, return_single=True):\n",
        "        super(ModelWrapper, self).__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.output_layer_names = output_layer_names\n",
        "        self.outputs = {}\n",
        "        self.return_single = return_single\n",
        "        add_hooks(self.model, self.outputs, self.output_layer_names)\n",
        "\n",
        "    def forward(self, images):\n",
        "        self.model(images)\n",
        "        output_vals = [self.outputs[output_layer_name] for output_layer_name in self.output_layer_names]\n",
        "        if self.return_single:\n",
        "            return output_vals[0]\n",
        "        else:\n",
        "            return output_vals\n",
        "\n",
        "\n",
        "class BBResNet18(object):\n",
        "    def __init__(self):\n",
        "        self.model = resnet18(pretrained=True)\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.eval()\n",
        "\n",
        "        self.model = ModelWrapper(self.model, ['avgpool'], True)\n",
        "\n",
        "        self.model.eval()\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def feature_extraction(self, x: np.ndarray):\n",
        "        '''\n",
        "            param:\n",
        "                x: numpy ndarray of shape: [None, 3, 224, 224] and dtype: np.float32\n",
        "            \n",
        "            return:\n",
        "                numpy ndarray (feature vector) of shape: [None, 512] and dtype: np.float32\n",
        "        '''\n",
        "\n",
        "        x = torch.from_numpy(x).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = self.model(x).cpu().detach()\n",
        "            out = out.view(out.size(0), -1)\n",
        "            out = out.numpy()\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "CeK7iwgaPSlC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utility Fuctions**"
      ],
      "metadata": {
        "id": "x1IM9HhcPZXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset():\n",
        "    print('Downloading CIFAR Dataset')\n",
        "    import tarfile\n",
        "    print('Download Complete. Extracting ...')\n",
        "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    file = tarfile.open(fileobj=response.raw, mode=\"r|gz\")\n",
        "    file.extractall(path=\".\")\n",
        "    print('Extraction Completed')\n",
        "\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "\n",
        "def download_url(url, output_path):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
        "\n",
        "# Function for checking the internet connectivity\n",
        "\n",
        "def connect(host='http://google.com'):\n",
        "    try:\n",
        "        urllib.request.urlopen(host)  # Python 3.x\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def url_exists(path):\n",
        "    r = requests.head(path)\n",
        "    return r.status_code == requests.codes.ok\n",
        "\n",
        "# Unpack the dataset using pickle library\n",
        "def unpickle(folder):\n",
        "    labels_mapping = {}\n",
        "    # np.set_printoptions(threshold=sys.maxsize)\n",
        "    train_data = {'data': np.array([]), 'labels': []}\n",
        "    test_data = {'data': np.array([]), 'labels': []}\n",
        "    print(os.listdir(folder))\n",
        "    for file in os.listdir(folder):\n",
        "        if file == \"data_batch_1\" or file == \"data_batch_2\" or file == \"data_batch_3\" or file == \"data_batch_4\" or file == \"data_batch_5\":\n",
        "            print('Currently Processing File : ', file)\n",
        "            dict = unpickle_each(os.path.join(folder, file))\n",
        "            if train_data['data'].shape[0] == 0:\n",
        "                train_data['data'] = dict[b'data']\n",
        "            else:\n",
        "                train_data['data'] = np.vstack([train_data['data'], dict[b'data']])\n",
        "            train_data['labels'] = train_data['labels'] + dict[b'labels']\n",
        "        elif file == 'batches.meta':\n",
        "            print('Currently Processing File : ', file)\n",
        "            labels_mapping = unpickle_each(os.path.join(folder, file))\n",
        "        elif file == 'test_batch':\n",
        "            print('Currently Processing File : ', file)\n",
        "            dict = unpickle_each(os.path.join(folder, file))\n",
        "            if test_data['data'].shape[0] == 0:\n",
        "                test_data['data'] = dict[b'data']\n",
        "            test_data['labels'] = test_data['labels'] + dict[b'labels']\n",
        "    return train_data, test_data, labels_mapping\n",
        "    # with open(file, 'rb') as fo:\n",
        "    #     dict = pickle.load(fo, encoding='bytes')\n",
        "    # return dict\n",
        "\n",
        "def unpickle_each(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "\n",
        "def preprocessing(data):\n",
        "    images = []\n",
        "    count = 1\n",
        "    for img in data['data']:\n",
        "        img_new = img.reshape((3, 32, 32))\n",
        "        # print(img_new[0][0][0])\n",
        "        image = np.transpose(img_new, [1, 2, 0])\n",
        "        images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "\n",
        "def img_enhancement(img):\n",
        "    # print(img)\n",
        "    arr_pxl = []\n",
        "    for i in range(img.shape[0]):\n",
        "        for j in range(img.shape[1]):\n",
        "            avg = np.average(img[i, j])\n",
        "            arr_pxl.append(avg)\n",
        "    imax = np.max(arr_pxl)\n",
        "    imin = np.min(arr_pxl)\n",
        "\n",
        "    for i in range(img.shape[0]):\n",
        "        for j in range(img.shape[1]):\n",
        "            pxl = []\n",
        "            avg = np.average(img[i, j])\n",
        "            ip = 255 * (avg - imin) / (imax - imin)\n",
        "            for c in range(img.shape[2]):\n",
        "                if img[i, j, c] == 0:\n",
        "                    n_pxl = 0\n",
        "                else:\n",
        "                    n_pxl = int((img[i, j, c] * ip / avg))\n",
        "                pxl.append(n_pxl)\n",
        "            img[i, j, :] = np.array(pxl)\n",
        "            # print(img.shape)\n",
        "    return img, img.shape\n",
        "\n",
        "\n",
        "def img_posterization(img):\n",
        "    imin = random.randint(30, 50)\n",
        "    imax = random.randint(100, 150)\n",
        "    range_pxl = abs(imax - imin)\n",
        "    divider = 255 / range_pxl\n",
        "\n",
        "    for i in range(img.shape[0]):\n",
        "        for j in range(img.shape[1]):\n",
        "            pxl = []\n",
        "            for c in range(img.shape[2]):\n",
        "                cont_pxl = (img[i, j, c] / divider)\n",
        "                cont_pxl = (cont_pxl + imin)\n",
        "                pxl.append(cont_pxl)\n",
        "            img[i, j, :] = np.array(pxl).clip(0, 255)\n",
        "    return img, img.shape\n",
        "\n",
        "\n",
        "# This function rotates the image around its center by random degree between [-180, 180].\n",
        "def random_rotation(image):\n",
        "    # Choose Random degree\n",
        "    degree = random.randint(-180, 180)\n",
        "    # print(\"Random degree chosen: \", degree)\n",
        "    # First we will convert the degrees into radians\n",
        "    rads = math.radians(degree)\n",
        "    cosine = math.cos(rads)\n",
        "    sine = math.sin(rads)\n",
        "\n",
        "    # Find the height and width of the rotated image using cosine and sine transformations\n",
        "    height_rot_img = round(abs(image.shape[0] * cosine)) + round(abs(image.shape[1] * sine))\n",
        "    width_rot_img = round(abs(image.shape[1] * cosine)) + round(abs(image.shape[0] * sine))\n",
        "\n",
        "    # Initialising the rotated image by zeros\n",
        "    rot_img = np.uint8(np.zeros((height_rot_img, width_rot_img, image.shape[2])))\n",
        "\n",
        "    # Finding the center point of the original image\n",
        "    orgx, orgy = (image.shape[1] // 2, image.shape[0] // 2)\n",
        "\n",
        "    # Finding the center point of rotated image.\n",
        "    rotx, roty = (width_rot_img // 2, height_rot_img // 2)\n",
        "\n",
        "    for i in range(rot_img.shape[0]):\n",
        "        for j in range(rot_img.shape[1]):\n",
        "            # Find the all new coordinates for orginal image wrt the new center point\n",
        "            x = (i - rotx) * cosine + (j - roty) * sine\n",
        "            y = -(i - rotx) * sine + (j - roty) * cosine\n",
        "\n",
        "            x = round(x) + orgy\n",
        "            y = round(y) + orgx\n",
        "\n",
        "            # Restricting the index in between original height and width of image.\n",
        "            if x >= 0 and y >= 0 and x < image.shape[0] and y < image.shape[1]:\n",
        "                rot_img[i, j, :] = image[x, y, :]\n",
        "    return rot_img, degree\n",
        "\n",
        "\n",
        "def contrast_and_flip(image):\n",
        "    img_height = image.shape[0]\n",
        "    img_width = image.shape[1]\n",
        "    # Adding two pixels padding across the image\n",
        "    img = np.uint8(np.zeros((img_height, img_width, image.shape[2])))\n",
        "\n",
        "    alpha = random.uniform(0.5, 2.0)\n",
        "    flip_prob = random.randint(0, 1)\n",
        "    # print(\"Alpha value: \", alpha)\n",
        "\n",
        "    for i in range(img.shape[0]):\n",
        "        for j in range(img.shape[1]):\n",
        "            pxl = []\n",
        "            for c in range(image.shape[2]):\n",
        "                cont_pxl = int(alpha * (image[i, j, c] - 128) + 128)\n",
        "                if cont_pxl > 255:\n",
        "                    cont_pxl = 255\n",
        "                elif cont_pxl < 0:\n",
        "                    cont_pxl = 0\n",
        "                pxl.append(cont_pxl)\n",
        "            img[i, j, :] = np.array(pxl)\n",
        "    if (flip_prob):\n",
        "        # print(\"Including Horizontal Flipping\")\n",
        "        img = img[:, ::-1, :]  # Horizontal Flipping\n",
        "    return img, round(alpha, 3)\n",
        "\n",
        "\n",
        "# Generating Augmented Images\n",
        "def get_augmented_images(data, labels):\n",
        "    augmented_img = []\n",
        "    augmented_labels = []\n",
        "    preprocess_func = {0: random_rotation, 1: img_enhancement, 2: img_posterization, 3: contrast_and_flip}\n",
        "    i = 0\n",
        "    # print(data)\n",
        "    for img in data:\n",
        "        rndm_idx = random.randint(0, 3)\n",
        "        if i % 1000 == 0:\n",
        "            print(\"\\nProcessing Image Number: \", i, end=' ')\n",
        "        # Resizing to restore rotated image's dimensions to 32 x 32\n",
        "        if preprocess_func[rndm_idx] != random_rotation:\n",
        "            n_img, _ = preprocess_func[rndm_idx](img)\n",
        "        else:\n",
        "            # print(img)\n",
        "            n_img, _ = preprocess_func[rndm_idx](img)\n",
        "            n_img = cv2.resize(n_img, (32, 32))\n",
        "        augmented_img.append(n_img)\n",
        "        augmented_labels.append(labels[i])\n",
        "        i += 1\n",
        "    return np.array(augmented_img), augmented_labels\n",
        "\n",
        "\n",
        "def get_feat_vec(images, obj):\n",
        "    feat_vec = []\n",
        "    count = 1\n",
        "    for img in images:\n",
        "        # print(count)\n",
        "        img = cv2.resize(img, (224, 224))\n",
        "        img = np.transpose(img, (2, 1, 0))\n",
        "        # Performing Normalization before sending into ResNet model\n",
        "        img = img / 255\n",
        "        img = np.array(img, dtype=np.float32)\n",
        "        feat_vec.append(obj.feature_extraction(np.array([img]))[0])\n",
        "        count += 1\n",
        "    return np.array(feat_vec)\n"
      ],
      "metadata": {
        "id": "plEDVWHTPcUO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download CIFAR-10 Dataset**"
      ],
      "metadata": {
        "id": "nOfvr0XUP8C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = Figlet(font='slant')\n",
        "print(f.renderText('CS774 Assignment 1'))\n",
        "\n",
        "f = Figlet(font='digital')\n",
        "print(f.renderText('by Anjali Manoj and Atanu Shuvam Roy'))\n",
        "\n",
        "# Check for internet\n",
        "print(\n",
        "    \"Connected to Internet. Ready for duty.\" if connect() else \"No Internet! Put Extracted Dataset in current directory\")\n",
        "\n",
        "dir = 'cifar-10-batches-py'\n",
        "file_exists = exists('cifar-10-python.tar.gz')\n",
        "dir_exists = exists(dir)\n",
        "if file_exists:\n",
        "    print('Previous File Exists. Removing File ...')\n",
        "    os.unlink('cifar-10-python.tar.gz')\n",
        "elif dir_exists:\n",
        "    files_required = ['batches.meta', 'data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5',\n",
        "                      'test_batch']\n",
        "    flag = True\n",
        "    for i in files_required:\n",
        "        path = Path(dir + f'/{i}')\n",
        "        if not path.is_file():\n",
        "            flag = False\n",
        "            break\n",
        "    if flag:\n",
        "        print('Dataset already available')\n",
        "    else:\n",
        "        print('Dataset Incomplete. Re-download required')\n",
        "        shutil.rmtree(dir)\n",
        "else:\n",
        "    print('Dataset Unavailable. Connecting to internet ...')\n",
        "    download_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMACdunkQEvd",
        "outputId": "5f92ba3a-366a-4f1d-89d3-3979298932ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   _______________________ __\n",
            "  / ____/ ___/__  /__  / // /\n",
            " / /    \\__ \\  / /  / / // /_\n",
            "/ /___ ___/ / / /  / /__  __/\n",
            "\\____//____/ /_/  /_/  /_/   \n",
            "                             \n",
            "    ___              _                                  __     ___\n",
            "   /   |  __________(_)___ _____  ____ ___  ___  ____  / /_   <  /\n",
            "  / /| | / ___/ ___/ / __ `/ __ \\/ __ `__ \\/ _ \\/ __ \\/ __/   / / \n",
            " / ___ |(__  |__  ) / /_/ / / / / / / / / /  __/ / / / /_    / /  \n",
            "/_/  |_/____/____/_/\\__, /_/ /_/_/ /_/ /_/\\___/_/ /_/\\__/   /_/   \n",
            "                   /____/                                         \n",
            "\n",
            "+-+-+ +-+-+-+-+-+-+ +-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+ +-+-+-+-+-+-+ +-+-+-+\n",
            "|b|y| |A|n|j|a|l|i| |M|a|n|o|j| |a|n|d| |A|t|a|n|u| |S|h|u|v|a|m| |R|o|y|\n",
            "+-+-+ +-+-+-+-+-+-+ +-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+ +-+-+-+-+-+-+ +-+-+-+\n",
            "\n",
            "Connected to Internet. Ready for duty.\n",
            "Dataset Unavailable. Connecting to internet ...\n",
            "Downloading CIFAR Dataset\n",
            "Download Complete. Extracting ...\n",
            "Extraction Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load The Dataset**"
      ],
      "metadata": {
        "id": "ISIBnDfnQPki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = Figlet(font='digital')\n",
        "print(f.renderText('Question 1: Loading the dataset'))\n",
        "\n",
        "train_data, test_data, labels_mapping = unpickle(dir)\n",
        "\n",
        "# np.set_printoptions(threshold=sys.maxsize)\n",
        "# print(train_data['data'][0])\n",
        "\n",
        "print(\"Total train data size:\", train_data['data'].shape)\n",
        "print(\"Total test data size:\", test_data['data'].shape)\n",
        "print(\"Labels available for CIFAR-10: \", labels_mapping[b'label_names'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIu8ScLwQSA8",
        "outputId": "74e074ff-5416-40c8-b13e-2c0df3699bd1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |1|:| |L|o|a|d|i|n|g| |t|h|e| |d|a|t|a|s|e|t|\n",
            "+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+-+-+\n",
            "\n",
            "['data_batch_5', 'data_batch_2', 'test_batch', 'data_batch_1', 'batches.meta', 'readme.html', 'data_batch_4', 'data_batch_3']\n",
            "Currently Processing File :  data_batch_5\n",
            "Currently Processing File :  data_batch_2\n",
            "Currently Processing File :  test_batch\n",
            "Currently Processing File :  data_batch_1\n",
            "Currently Processing File :  batches.meta\n",
            "Currently Processing File :  data_batch_4\n",
            "Currently Processing File :  data_batch_3\n",
            "Total train data size: (50000, 3072)\n",
            "Total test data size: (10000, 3072)\n",
            "Labels available for CIFAR-10:  [b'airplane', b'automobile', b'bird', b'cat', b'deer', b'dog', b'frog', b'horse', b'ship', b'truck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Image Processing/Augmentation**"
      ],
      "metadata": {
        "id": "1UXSvUQ2QYYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2: Image transformations\n",
        "\n",
        "f = Figlet(font='digital')\n",
        "print(f.renderText('Question 2 (a): Image Enhancement'))\n",
        "\n",
        "org_train_images = preprocessing(train_data)\n",
        "org_test_images = preprocessing(test_data)\n",
        "\n",
        "# Test Image for applying image enhancement\n",
        "first_image = org_train_images[0]\n",
        "\n",
        "# Matplotlib Backend Specify to ignore the bug (MacOSX)\n",
        "# mpl_use('MacOSX')\n",
        "plt.figure(figsize=(2, 2))\n",
        "# plt.imshow(first_image)\n",
        "# plt.show()\n",
        "\n",
        "# Question 2 (a)\n",
        "plt.figure(figsize=(2, 2))\n",
        "first_image = org_train_images[random.randint(0, 49000)]\n",
        "enhanced_image, _ = img_enhancement(first_image)\n",
        "plt.imshow(enhanced_image)\n",
        "plt.show()\n",
        "\n",
        "# Question 2 (b)\n",
        "print(f.renderText('Question 2 (b): Posterization of Image'))\n",
        "first_image = org_train_images[random.randint(0, 49000)]\n",
        "posterized_image, _ = img_posterization(first_image)\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(posterized_image)\n",
        "plt.show()\n",
        "\n",
        "# Question 2 (c)\n",
        "print(f.renderText('Question 2 (c): Random Rotate'))\n",
        "first_image = org_train_images[random.randint(0, 49000)]\n",
        "rotated_image, rotated_degree = random_rotation(first_image)\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(rotated_image)\n",
        "plt.show()\n",
        "\n",
        "print(f.renderText('Question 2 (d): Contrast and Horizontal Flipping'))\n",
        "first_image = org_train_images[random.randint(0, 49000)]\n",
        "contrast_image, alpha = contrast_and_flip(first_image)\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(contrast_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "uvpnlpt0QjF2",
        "outputId": "e3056fe8-dbc2-4b98-8b77-e49cdb231e05"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+ +-+-+-+-+-+-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |2| |(|a|)|:| |I|m|a|g|e| |E|n|h|a|n|c|e|m|e|n|t|\n",
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+ +-+-+-+-+-+-+-+-+-+-+-+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 144x144 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |2| |(|b|)|:| |P|o|s|t|e|r|i|z|a|t|i|o|n| |o|f| |I|m|a|g|e|\n",
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+-+ +-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |2| |(|c|)|:| |R|a|n|d|o|m| |R|o|t|a|t|e|\n",
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+-+ +-+-+-+-+-+-+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |2| |(|d|)|:| |C|o|n|t|r|a|s|t| |a|n|d| |H|o|r|i|z|o|n|t|a|l|\n",
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+-+-+-+-+-+\n",
            "+-+-+-+-+-+-+-+-+\n",
            "|F|l|i|p|p|i|n|g|\n",
            "+-+-+-+-+-+-+-+-+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Augmented Image Set**"
      ],
      "metadata": {
        "id": "pCQcnm2-R7yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f.renderText('Question 3: Augmented Images'))\n",
        "\n",
        "# Generating Augmented Images\n",
        "cifar_augmented_dir = './cifar-10-batches-augmented-py'\n",
        "# Check whether the specified path exists or not\n",
        "isExist = os.path.exists(cifar_augmented_dir)\n",
        "if not isExist:\n",
        "    # Create a new directory because it does not exist\n",
        "    os.makedirs(cifar_augmented_dir)\n",
        "    print(\"The new directory (cifar-10-batches-augmented-py) is created!\")\n",
        "\n",
        "print('Checking for preprocessed augmented data')\n",
        "if os.path.exists(cifar_augmented_dir + '/augmented_batch.npy') and os.path.exists(cifar_augmented_dir + '/augmented_batch_labels.npy'):\n",
        "    print('Preprocessed augmented data found. Loading Data')\n",
        "    augmented_train_set = np.load(cifar_augmented_dir + '/augmented_batch.npy')\n",
        "    augmented_train_labels = np.load(cifar_augmented_dir + '/augmented_batch_labels.npy')\n",
        "else:\n",
        "    print('Preprocessed augmented data NOT found. Checking Online ...')\n",
        "    aug_batch_url =\"https://cse.iitk.ac.in/users/atanusroy/augmented_batch.npy\";\n",
        "    aug_batch_labels_url =\"https://cse.iitk.ac.in/users/atanusroy/augmented_batch_labels.npy\";\n",
        "    if(url_exists(aug_batch_url) and url_exists(aug_batch_labels_url)):\n",
        "      download_url(aug_batch_url, cifar_augmented_dir + '/augmented_batch.npy')\n",
        "      download_url(aug_batch_labels_url, cifar_augmented_dir + '/augmented_batch_labels.npy')\n",
        "      augmented_train_set = np.load(cifar_augmented_dir + '/augmented_batch.npy')\n",
        "      augmented_train_labels = np.load(cifar_augmented_dir + '/augmented_batch_labels.npy')\n",
        "    else:\n",
        "      print('File Not Available Online. Regenerating Augmented Dataset ...')\n",
        "      train_augmented_img, train_augmented_labels = get_augmented_images(org_train_images, train_data['labels'])\n",
        "      augmented_train_set = np.vstack([org_train_images, train_augmented_img])\n",
        "      augmented_train_labels = train_data['labels'] + train_augmented_labels\n",
        "      np.save(cifar_augmented_dir + '/augmented_batch.npy', augmented_train_set)\n",
        "      np.save(cifar_augmented_dir + '/augmented_batch_labels.npy', augmented_train_labels)\n",
        "      print('Augmented Data Shape: ', train_augmented_img.shape)\n",
        "    print(\"Original Data Shape: \", org_train_images.shape)\n",
        "print('Size of New Training Data Set: ', len(augmented_train_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkgSjTc6SDl-",
        "outputId": "81268a14-feac-4b2a-8ee3-fc8ad07118ba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+-+-+-+-+ +-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |3|:| |A|u|g|m|e|n|t|e|d| |I|m|a|g|e|s|\n",
            "+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+-+-+-+-+ +-+-+-+-+-+-+\n",
            "\n",
            "The new directory (cifar-10-batches-augmented-py) is created!\n",
            "Checking for preprocessed augmented data\n",
            "Preprocessed augmented data NOT found. Checking Online ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "augmented_batch.npy: 307MB [05:57, 859kB/s]                           \n",
            "augmented_batch_labels.npy: 803kB [00:00, 1.01MB/s]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Shape:  (50000, 32, 32, 3)\n",
            "Size of New Training Data Set:  100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get Feature Vectors**"
      ],
      "metadata": {
        "id": "pGQmZMO9U55A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f.renderText('Question 4: Feature Vector'))\n",
        "\n",
        "obj = BBResNet18()\n",
        "\n",
        "feature_vec_dir = './feature_vectors'\n",
        "# Check whether the specified path exists or not\n",
        "isExist = os.path.exists(feature_vec_dir)\n",
        "if not isExist:\n",
        "    # Create a new directory because it does not exist\n",
        "    os.makedirs(feature_vec_dir)\n",
        "    print(\"The new directory (feature_vec_dir) is created!\")\n",
        "\n",
        "if exists('./feature_vectors/original_train_feature_vector.npy') and exists(\n",
        "        './feature_vectors/original_test_feature_vector.npy'):\n",
        "    print(\"Loading Original Data Training Feature Vector\")\n",
        "\n",
        "    original_train_feat_vec = np.load('./feature_vectors/original_train_feature_vector.npy')\n",
        "    original_test_feat_vec = np.load('./feature_vectors/original_test_feature_vector.npy')\n",
        "else:\n",
        "    print(\"Generating Original Data Training Feature Vector\")\n",
        "\n",
        "    original_train_feat_vec = get_feat_vec(org_train_images, obj)\n",
        "    original_test_feat_vec = get_feat_vec(org_test_images, obj)\n",
        "\n",
        "    np.save('./feature_vectors/original_train_feature_vector.npy', original_train_feat_vec)\n",
        "    np.save('./feature_vectors/original_test_feature_vector.npy', original_test_feat_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "86b6c6b17f3f47ab899f4777d785abe0",
            "6539af09cb564c07802c5cb79661d693",
            "9b15ca9083e44b938b9700632f9ce31d",
            "5234d5e7c20e4b868849d28dcd1e7512",
            "321f87bb4d254d468abdc1cab1af5c0c",
            "0e0960322cf64fe1a12a251394b6b486",
            "4e973c47fce24a25acd6bb3229c4d1b4",
            "f910419696e9444ebdd2a9d05f37794a",
            "80839d032c064836a2373d6afe9d5012",
            "48d20ee3cd904f81b64dcd796298b638",
            "1be4dc216f28483d86bb4d6c1b508e31"
          ]
        },
        "id": "T80pYhWUU8y5",
        "outputId": "5a28c0e5-e566-431a-e0da-9ae68d9064a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+-+-+ +-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |4|:| |F|e|a|t|u|r|e| |V|e|c|t|o|r|\n",
            "+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+-+-+ +-+-+-+-+-+-+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86b6c6b17f3f47ab899f4777d785abe0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new directory (feature_vec_dir) is created!\n",
            "Generating Original Data Training Feature Vector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MLP IMPLEMENTATION**"
      ],
      "metadata": {
        "id": "FEOGTc1VVOar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
        "\n",
        "def relu(x, deriv=False):\n",
        "    # ReLU activation function\n",
        "    if (deriv):\n",
        "        return np.where(x <= 0, 0, 1)\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    '''\n",
        "        softmax(x) = exp(x) / sum(exp(x))\n",
        "    '''\n",
        "\n",
        "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    prob = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "    return prob\n",
        "\n",
        "\n",
        "def cross_entropy_loss(predicted, target):\n",
        "    \"\"\"\n",
        "    predicted (batch_size x num_classes)\n",
        "    target is labels (batch_size x 1)\n",
        "    \"\"\"\n",
        "    log_likelihood = -np.log(predicted[range(target.shape[0]), target])\n",
        "    loss = -np.sum(log_likelihood) / target.shape[0]\n",
        "    # print(\"Loss: \", np.sum(loss))\n",
        "    # exit()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def get_one_hot_vector(y):\n",
        "    \"\"\"\n",
        "    generating one hot vector for class labels\n",
        "    \"\"\"\n",
        "    y = np.array(y)\n",
        "    y_one_hot = np.zeros((y.shape[0], 10))\n",
        "    y_one_hot[np.arange(y.shape[0]), y] = 1\n",
        "    # y_one_hot = y_one_hot.reshape(-1, 10, 1)\n",
        "    return y_one_hot\n",
        "\n",
        "\n",
        "class MLP(object):\n",
        "    def __init__(self, input_size, load_model_weights, augmented):\n",
        "        if load_model_weights:\n",
        "            print(\"Loading from pretrained model\")\n",
        "            if (augmented):\n",
        "                read_weights = np.load(load_model_weights + '/augmented-model_weights.npy', allow_pickle='TRUE').item()\n",
        "            else:\n",
        "                read_weights = np.load(load_model_weights + '/unaugmented-model_weights.npy',\n",
        "                                       allow_pickle='TRUE').item()\n",
        "            self.weights = read_weights['weights']\n",
        "            self.biases = read_weights['biases']\n",
        "        else:\n",
        "            print(\"Initialising weights and biases\")\n",
        "\n",
        "            self.weights = [np.random.randn(y, x) * 0.1 for x, y in\n",
        "                            zip(input_size[:-2], input_size[1:-1])] + [\n",
        "                               np.random.randn(input_size[-1], input_size[-2]) * 0.1]\n",
        "            self.biases = [np.zeros((x, 1)) for x in input_size[1:-1]] + [np.zeros((input_size[-1], 1))]\n",
        "\n",
        "        print(\"Weights Shape for Input Layer: \", self.weights[0].shape)\n",
        "        print(\"Weights Shape for Hidden Layer 1: \", self.weights[1].shape)\n",
        "        print(\"Weights Shape for Hidden Layer 2: \", self.weights[2].shape)\n",
        "\n",
        "        print(\"Bias Shape for Input Layer: \", self.biases[0].shape)\n",
        "        print(\"Bias Shape for Hidden Layer 1: \", self.biases[1].shape)\n",
        "        print(\"Bias Shape for Hidden Layer 2: \", self.biases[2].shape)\n",
        "\n",
        "    def feedforward(self, x):\n",
        "\n",
        "        z1 = np.dot(self.weights[0], x) + self.biases[0][:, :x.shape[1]]\n",
        "        a1 = relu(z1)\n",
        "        z2 = np.dot(self.weights[1], a1) + self.biases[1][:, :a1.shape[1]]\n",
        "        a2 = relu(z2)\n",
        "        z3 = np.dot(self.weights[2], a2) + self.biases[2][:, :a2.shape[1]]\n",
        "        a3 = softmax(z3.T)\n",
        "\n",
        "        return z1, a1, z2, a2, z3, a3\n",
        "\n",
        "    def backpropagation(self, X, y):\n",
        "\n",
        "        delta_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        delta_b = [np.zeros(b.shape) for b in self.biases]\n",
        "\n",
        "        z1, a1, z2, a2, z3, a3 = self.feedforward(X)\n",
        "        y = y.argmax(axis=1)\n",
        "        loss = cross_entropy_loss(a3, y)\n",
        "        a3[range(y.shape[0]), y] -= 1\n",
        "        error = a3\n",
        "\n",
        "        # For Output Layer\n",
        "        delta1 = error.T\n",
        "        delta_b[2] = delta1\n",
        "        delta_w[2] = np.dot(delta1, a2.T)\n",
        "\n",
        "        # For Second Hidden Layer\n",
        "        deriv_relu = relu(z2, deriv=True)\n",
        "        delta2 = np.dot(self.weights[2].T, delta1) * deriv_relu\n",
        "        delta_b[1] = delta2\n",
        "        delta_w[1] = np.dot(delta2, a1.T)\n",
        "\n",
        "        # For First Hidden Layer\n",
        "        deriv_relu = relu(z1, deriv=True)\n",
        "        delta3 = np.dot(self.weights[1].T, delta2) * deriv_relu\n",
        "        delta_b[0] = delta3\n",
        "        delta_w[0] = np.dot(delta3, X.T)\n",
        "\n",
        "        return loss, delta_b, delta_w\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "\n",
        "        count = 0\n",
        "        preds = np.array([])\n",
        "        for x, _y in zip(X, y):\n",
        "            _, _, _, _, _, a3 = self.feedforward(np.array([x]).T)\n",
        "            preds = np.append(preds, np.argmax(a3))\n",
        "            if np.argmax(a3) == np.argmax(_y):\n",
        "                count += 1\n",
        "        return float(count) / X.shape[0], preds\n",
        "\n",
        "    def train(self, X, y, X_test, y_test, learning_rate=0.01, epochs=5, batch_size=100):\n",
        "        history_training_loss, history_training_acc, history_test_acc, history_test_pred = [], [], [], []\n",
        "        model_weights = {}\n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle\n",
        "            permutation = np.random.permutation(X.shape[0])\n",
        "            x_train_shuffled = X[permutation]\n",
        "            y_train_shuffled = y[permutation]\n",
        "\n",
        "            # same shape as self.biases\n",
        "            del_b = [np.zeros(b.shape) for b in self.biases]\n",
        "            # same shape as self.weights\n",
        "            del_w = [np.zeros(w.shape) for w in self.weights]\n",
        "            flag = 0\n",
        "            loss_min = 9999\n",
        "\n",
        "            for batch_idx in range(0, X.shape[0], batch_size):\n",
        "                # Selecting batches of the training images\n",
        "                batch_x = x_train_shuffled[batch_idx:batch_idx + batch_size]\n",
        "                batch_y = y_train_shuffled[batch_idx:batch_idx + batch_size]\n",
        "\n",
        "                # Performing Backpropagation\n",
        "                loss, delta_b, delta_w = self.backpropagation(batch_x.T, batch_y)\n",
        "                loss = abs(loss)\n",
        "                # Maintaining the original shape according to batch size when input is less than given batch size\n",
        "                if delta_b[0].shape[1] < batch_size:\n",
        "                    # print(delta_b[0].shape) \n",
        "                    size_left = batch_size - delta_b[0].shape[1]\n",
        "                    delta_b[0] = np.pad(delta_b[0], ((0, 0), (0, size_left)))\n",
        "\n",
        "                if delta_b[1].shape[1] < batch_size:\n",
        "                    # print(delta_b[1].shape)\n",
        "                    size_left = batch_size - delta_b[1].shape[1]\n",
        "                    delta_b[1] = np.pad(delta_b[1], ((0, 0), (0, size_left)))\n",
        "\n",
        "                if delta_b[2].shape[1] < batch_size:\n",
        "                    # print(delta_b[2].shape)\n",
        "                    size_left = batch_size - delta_b[2].shape[1]\n",
        "                    delta_b[2] = np.pad(delta_b[2], ((0, 0), (0, size_left)))\n",
        "\n",
        "                #     #It will encounter only when there's minimum loss is getting as compared to previous batches\n",
        "                if loss < loss_min:\n",
        "                    loss_min = loss\n",
        "                    del_b = [db + ddb for db, ddb in zip(del_b, delta_b)]\n",
        "                    del_w = [dw + ddw for dw, ddw in zip(del_w, delta_w)]\n",
        "\n",
        "            self.weights = [w - (learning_rate / batch_size) * dw for w, dw in zip(self.weights, del_w)]\n",
        "            self.biases = [b - (learning_rate / batch_size) * db for b, db in zip(self.biases, del_b)]\n",
        "\n",
        "            model_weights['weights'] = self.weights\n",
        "            model_weights['biases'] = self.biases\n",
        "\n",
        "            # Evaluate performance\n",
        "            train_acc, _ = self.evaluate(X, y)\n",
        "            test_acc, test_pred = self.evaluate(X_test, y_test)\n",
        "            history_training_loss.append(loss_min)\n",
        "            history_training_acc.append(train_acc)\n",
        "            history_test_acc.append(test_acc)\n",
        "            history_test_pred.append(test_pred)\n",
        "            print(\"Epoch: %d Training loss: %.3f Training accuracy: %.2f Testing Accuracy: %.2f\" % (\n",
        "                epoch, loss_min, train_acc * 100, test_acc * 100))\n",
        "        return np.array(history_training_acc), np.array(history_training_loss), np.array(history_test_acc), np.array(\n",
        "            history_test_pred), model_weights\n",
        "\n",
        "\n",
        "def Model(X_train, y_train, X_test, y_test, model_wt_folder, out_folder, isModelWeightsAvailable=0, epochs=500,\n",
        "          batch_size=32, learning_rate=0.01, augmented=False):\n",
        "    inp_feats = 512\n",
        "    num_neurons_1 = 64\n",
        "    num_neurons_2 = 64\n",
        "    num_output = 10\n",
        "    # batch_size=256\n",
        "    # learning_rate=0.01\n",
        "    print(\"Epochs: \", epochs)\n",
        "    print(\"y_train: \", y_train.shape)\n",
        "    print(\"batch-size: \", batch_size)\n",
        "    print(\"learning_rate: \", learning_rate)\n",
        "    if isModelWeightsAvailable:\n",
        "        print(\"Performance measure on test datasets\")\n",
        "        model = MLP((inp_feats, num_neurons_1, num_neurons_2, num_output), model_wt_folder, augmented)\n",
        "        acc, prediction = model.evaluate(X_test, y_test)\n",
        "        print(\"Testing Accuracy or Performance Measure in percent: %.2f\" % (acc * 100))\n",
        "    else:\n",
        "        model = MLP((inp_feats, num_neurons_1, num_neurons_2, num_output), '', augmented)\n",
        "        total_training_acc, total_training_loss, total_testing_acc, total_test_pred, model_weights = model.train(\n",
        "            X_train, y_train, X_test, y_test, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)\n",
        "        # print(model_weights['weights'].shape)\n",
        "        if (augmented):\n",
        "            np.save(model_wt_folder + '/augmented-model_weights.npy', model_weights)\n",
        "        else:\n",
        "            np.save(model_wt_folder + '/unaugmented-model_weights.npy', model_weights)\n",
        "        # acc = model.evaluate(X_test, y_test)\n",
        "        print(\"Testing Accuracy or Performance Measure in percent: %.2f at epoch: %d\" % (\n",
        "            total_testing_acc[np.argmax(total_training_acc)] * 100, np.argmax(total_training_acc) + 1))\n",
        "        print(\"With a loss: %.3f\" % (total_training_loss[np.argmax(total_training_acc)]))\n",
        "        prediction = total_test_pred[np.argmax(total_training_acc)]\n",
        "\n",
        "        output_dir = './output'\n",
        "        # Check whether the specified path exists or not\n",
        "        isExist = os.path.exists(output_dir)\n",
        "        if not isExist:\n",
        "            # Create a new directory because it does not exist\n",
        "            os.makedirs(output_dir)\n",
        "            print(\"The new directory (output) is created!\")\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.plot(np.arange(epochs), total_training_acc, label='Total_trainig_acc')\n",
        "        plt.plot(np.arange(epochs), total_training_loss, label='Total_training_loss')\n",
        "        plt.plot(np.arange(epochs), total_testing_acc, label='Total_testing_acc')\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Training Accuracy and Loss\")\n",
        "        plt.title(\n",
        "            \"Loss/Accuracy vs Epochs - \" + \"batch_size-\" + str(batch_size) + \"-learning-rate-\" + str(learning_rate))\n",
        "        plt.legend()\n",
        "        if (augmented):\n",
        "            plt.savefig(out_folder + \"/augmented-loss-accuracy-graph-\" + str(batch_size) + \"-\" + str(\n",
        "                learning_rate) + \".jpg\")\n",
        "        else:\n",
        "            plt.savefig(\n",
        "                out_folder + \"/unaugmented-loss-accuracy-graph-\" + str(batch_size) + \"-\" + str(learning_rate) + \".jpg\")\n",
        "\n",
        "    # Y_test = y_test.argmax(axis=1)\n",
        "    # print('Accuracy: %.3f' % (accuracy_score(Y_test, prediction)*100))\n",
        "    # print('Precision: %.3f' % precision_score(Y_test, prediction, average='weighted'))\n",
        "    # print('Recall: %.3f'% recall_score(Y_test, prediction, average='weighted'))\n",
        "    # print('F1 score: %.3f'% f1_score(Y_test, prediction, average='weighted'))\n",
        "    # print('confussion matrix:\\n',confusion_matrix(Y_test, prediction))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ISJOtwGWV5c4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Call MLP**"
      ],
      "metadata": {
        "id": "6fNyGALlViHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f.renderText('Question 5 & 6(a): MLP Implementation'))\n",
        "\n",
        "model_weights_dir = './model_weights'\n",
        "# Check whether the specified path exists or not\n",
        "isExist = os.path.exists(model_weights_dir)\n",
        "if not isExist:\n",
        "    # Create a new directory because it does not exist\n",
        "    os.makedirs(model_weights_dir)\n",
        "    print(\"The new directory (model_weights) is created!\")\n",
        "\n",
        "output_dir = './output'\n",
        "# Check whether the specified path exists or not\n",
        "isExist = os.path.exists(output_dir)\n",
        "if not isExist:\n",
        "    # Create a new directory because it does not exist\n",
        "    os.makedirs(output_dir)\n",
        "    print(\"The new directory (output) is created!\")\n",
        "\n",
        "labels = np.arange(10)\n",
        "print(\"Training on Un-Augmented Datasets\")\n",
        "train_labels = get_one_hot_vector(train_data['labels'])\n",
        "test_labels = get_one_hot_vector(test_data['labels'])\n",
        "unaugmented_model = Model(original_train_feat_vec, train_labels, original_test_feat_vec, test_labels, './model_weights',\n",
        "                          './output', isModelWeightsAvailable=0, epochs=500, batch_size=128, learning_rate=0.01,\n",
        "                          augmented=False)\n",
        "torch.save(unaugmented_model, './models/unaugmented_model')\n",
        "print(f.renderText('Question 6 (b): Back Propagation'))\n",
        "\n",
        "print(\"Checking Augmented Data Training Feature Vector\")\n",
        "\n",
        "if exists('./feature_vectors/augmented_train_feature_vector.npy'):\n",
        "    print(\"Loading Augmented Data Training Feature Vector\")\n",
        "\n",
        "    augmented_train_feat_vec = np.load('./feature_vectors/original_train_feature_vector.npy')\n",
        "else:\n",
        "    print(\"Generating Augmented Data Training Feature Vector\")\n",
        "    augmented_train_feat_vec = get_feat_vec(augmented_train_set, obj)\n",
        "    np.save('./feature_vectors/augmented_train_feature_vector.npy', augmented_train_feat_vec)\n",
        "\n",
        "print(\"Training on Augmented Datasets\")\n",
        "\n",
        "model_dir = './models'\n",
        "# Check whether the specified path exists or not\n",
        "isExist = os.path.exists(model_dir)\n",
        "if not isExist:\n",
        "    # Create a new directory because it does not exist\n",
        "    os.makedirs(model_dir)\n",
        "    print(\"The new directory (models) is created!\")\n",
        "    \n",
        "\n",
        "aug_train_labels = get_one_hot_vector(augmented_train_labels)\n",
        "augmented_model = Model(augmented_train_feat_vec, aug_train_labels, original_test_feat_vec, test_labels,\n",
        "                        './model_weights',\n",
        "                        './output', isModelWeightsAvailable=0, epochs=50, batch_size=128, learning_rate=0.01,\n",
        "                        augmented=True)\n",
        "torch.save(augmented_model, './models/augmented_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0nLlv3vVlJ2",
        "outputId": "77668e51-20be-4cb4-8878-2ef33a7a637f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+ +-+ +-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |5| |&| |6|(|a|)|:| |M|L|P| |I|m|p|l|e|m|e|n|t|a|t|i|o|n|\n",
            "+-+-+-+-+-+-+-+-+ +-+ +-+ +-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
            "\n",
            "Training on Un-Augmented Datasets\n",
            "Epochs:  500\n",
            "y_train:  (50000, 10)\n",
            "batch-size:  128\n",
            "learning_rate:  0.01\n",
            "Initialising weights and biases\n",
            "Weights Shape for Input Layer:  (64, 512)\n",
            "Weights Shape for Hidden Layer 1:  (64, 64)\n",
            "Weights Shape for Hidden Layer 2:  (10, 64)\n",
            "Bias Shape for Input Layer:  (64, 1)\n",
            "Bias Shape for Hidden Layer 1:  (64, 1)\n",
            "Bias Shape for Hidden Layer 2:  (10, 1)\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 0 Training loss: 2.399 Training accuracy: 14.05 Testing Accuracy: 13.78\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 1 Training loss: 2.210 Training accuracy: 14.81 Testing Accuracy: 14.37\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 2 Training loss: 2.182 Training accuracy: 16.94 Testing Accuracy: 16.82\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 3 Training loss: 2.156 Training accuracy: 19.19 Testing Accuracy: 19.29\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 4 Training loss: 2.107 Training accuracy: 20.06 Testing Accuracy: 19.88\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 5 Training loss: 2.107 Training accuracy: 21.40 Testing Accuracy: 21.35\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 6 Training loss: 2.078 Training accuracy: 22.08 Testing Accuracy: 21.95\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 7 Training loss: 2.048 Training accuracy: 22.32 Testing Accuracy: 22.21\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 8 Training loss: 2.039 Training accuracy: 22.78 Testing Accuracy: 22.83\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 9 Training loss: 2.049 Training accuracy: 23.86 Testing Accuracy: 23.67\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 10 Training loss: 2.028 Training accuracy: 24.04 Testing Accuracy: 24.10\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 11 Training loss: 1.992 Training accuracy: 24.27 Testing Accuracy: 24.08\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 12 Training loss: 1.949 Training accuracy: 27.47 Testing Accuracy: 27.48\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 13 Training loss: 1.895 Training accuracy: 28.03 Testing Accuracy: 28.22\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 14 Training loss: 1.950 Training accuracy: 28.80 Testing Accuracy: 29.22\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 15 Training loss: 1.908 Training accuracy: 29.11 Testing Accuracy: 29.32\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 16 Training loss: 1.913 Training accuracy: 29.59 Testing Accuracy: 29.69\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 17 Training loss: 1.904 Training accuracy: 29.29 Testing Accuracy: 29.30\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 18 Training loss: 1.846 Training accuracy: 30.72 Testing Accuracy: 30.89\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 19 Training loss: 1.844 Training accuracy: 31.03 Testing Accuracy: 31.17\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 20 Training loss: 1.836 Training accuracy: 32.29 Testing Accuracy: 32.02\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 21 Training loss: 1.840 Training accuracy: 31.34 Testing Accuracy: 31.29\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 22 Training loss: 1.804 Training accuracy: 32.97 Testing Accuracy: 32.93\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 23 Training loss: 1.720 Training accuracy: 32.85 Testing Accuracy: 32.62\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 24 Training loss: 1.742 Training accuracy: 32.92 Testing Accuracy: 32.87\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 25 Training loss: 1.755 Training accuracy: 34.40 Testing Accuracy: 34.79\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 26 Training loss: 1.729 Training accuracy: 35.61 Testing Accuracy: 35.35\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 27 Training loss: 1.672 Training accuracy: 34.51 Testing Accuracy: 34.87\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 28 Training loss: 1.675 Training accuracy: 34.60 Testing Accuracy: 34.67\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 29 Training loss: 1.690 Training accuracy: 33.14 Testing Accuracy: 32.93\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 30 Training loss: 1.694 Training accuracy: 36.04 Testing Accuracy: 35.56\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 31 Training loss: 1.671 Training accuracy: 35.83 Testing Accuracy: 36.04\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 32 Training loss: 1.631 Training accuracy: 34.51 Testing Accuracy: 34.12\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 33 Training loss: 1.634 Training accuracy: 35.90 Testing Accuracy: 35.79\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 34 Training loss: 1.614 Training accuracy: 33.25 Testing Accuracy: 33.37\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 35 Training loss: 1.627 Training accuracy: 33.76 Testing Accuracy: 33.20\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 36 Training loss: 1.595 Training accuracy: 35.97 Testing Accuracy: 35.51\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 37 Training loss: 1.575 Training accuracy: 38.13 Testing Accuracy: 37.93\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 38 Training loss: 1.524 Training accuracy: 38.33 Testing Accuracy: 38.59\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 39 Training loss: 1.548 Training accuracy: 38.69 Testing Accuracy: 39.05\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 40 Training loss: 1.535 Training accuracy: 36.74 Testing Accuracy: 36.90\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 41 Training loss: 1.544 Training accuracy: 37.64 Testing Accuracy: 37.48\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 42 Training loss: 1.499 Training accuracy: 39.76 Testing Accuracy: 39.83\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 43 Training loss: 1.521 Training accuracy: 40.80 Testing Accuracy: 40.69\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 44 Training loss: 1.508 Training accuracy: 40.67 Testing Accuracy: 40.65\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 45 Training loss: 1.490 Training accuracy: 39.73 Testing Accuracy: 39.72\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 46 Training loss: 1.499 Training accuracy: 39.62 Testing Accuracy: 39.39\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 47 Training loss: 1.482 Training accuracy: 40.10 Testing Accuracy: 40.11\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 48 Training loss: 1.439 Training accuracy: 30.61 Testing Accuracy: 30.11\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 49 Training loss: 1.530 Training accuracy: 31.83 Testing Accuracy: 31.73\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 50 Training loss: 1.645 Training accuracy: 35.27 Testing Accuracy: 35.54\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 51 Training loss: 1.500 Training accuracy: 41.28 Testing Accuracy: 41.52\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 52 Training loss: 1.433 Training accuracy: 39.73 Testing Accuracy: 39.23\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 53 Training loss: 1.382 Training accuracy: 42.55 Testing Accuracy: 41.93\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 54 Training loss: 1.389 Training accuracy: 42.35 Testing Accuracy: 42.36\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 55 Training loss: 1.396 Training accuracy: 42.23 Testing Accuracy: 42.28\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 56 Training loss: 1.321 Training accuracy: 42.87 Testing Accuracy: 42.55\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 57 Training loss: 1.411 Training accuracy: 43.61 Testing Accuracy: 43.52\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 58 Training loss: 1.382 Training accuracy: 42.83 Testing Accuracy: 42.08\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 59 Training loss: 1.396 Training accuracy: 42.80 Testing Accuracy: 42.86\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 60 Training loss: 1.346 Training accuracy: 42.11 Testing Accuracy: 41.38\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 61 Training loss: 1.373 Training accuracy: 42.34 Testing Accuracy: 42.25\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 62 Training loss: 1.380 Training accuracy: 29.66 Testing Accuracy: 29.56\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 63 Training loss: 1.548 Training accuracy: 37.22 Testing Accuracy: 36.99\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 64 Training loss: 1.540 Training accuracy: 38.38 Testing Accuracy: 38.34\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 65 Training loss: 1.483 Training accuracy: 44.77 Testing Accuracy: 44.69\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 66 Training loss: 1.328 Training accuracy: 43.52 Testing Accuracy: 43.19\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 67 Training loss: 1.309 Training accuracy: 44.86 Testing Accuracy: 44.76\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 68 Training loss: 1.362 Training accuracy: 44.60 Testing Accuracy: 44.78\n",
            "(64, 80)\n",
            "(64, 80)\n",
            "Epoch: 69 Training loss: 1.323 Training accuracy: 43.58 Testing Accuracy: 43.16\n",
            "(64, 80)\n",
            "(64, 80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Load for Classification**\n",
        "Must Run for Classification"
      ],
      "metadata": {
        "id": "r51C-wYokuPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import platform\n",
        "\n",
        "def load_pickle(f):\n",
        "    version = platform.python_version_tuple()\n",
        "    if version[0] == '2':\n",
        "        return  pickle.load(f)\n",
        "    elif version[0] == '3':\n",
        "        return  pickle.load(f, encoding='latin1')\n",
        "    raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = load_pickle(f)\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "  \"\"\" load all of cifar \"\"\"\n",
        "  xs = []\n",
        "  ys = []\n",
        "  for b in range(1,6):\n",
        "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "    X, Y = load_CIFAR_batch(f)\n",
        "    xs.append(X)\n",
        "    ys.append(Y)\n",
        "  Xtr = np.concatenate(xs)\n",
        "  Ytr = np.concatenate(ys)\n",
        "  del X, Y\n",
        "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "  return Xtr, Ytr, Xte, Yte"
      ],
      "metadata": {
        "id": "arQKbyBzkydC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **KNN Classification**"
      ],
      "metadata": {
        "id": "8bwRBnwpkQhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification\n",
        "# KNearest Neighbor Classification\n",
        "\n",
        "\n",
        "class KNearestNeighbor(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def train(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X, k=1, num_loops=0):\n",
        "        if num_loops == 0:\n",
        "            dists = self.compute_distances(X)\n",
        "        else:\n",
        "            raise ValueError('Invalid value %d for num_loops' % num_loops)\n",
        "        return self.predict_labels(dists, k=k)\n",
        "\n",
        "    def compute_distances(self, X):\n",
        "        num_test = X.shape[0]\n",
        "        num_train = self.X_train.shape[0]\n",
        "        dists = np.zeros((num_test, num_train))\n",
        "        dists = np.sqrt(\n",
        "            np.sum(np.square(self.X_train), axis=1) + np.sum(np.square(X), axis=1)[:, np.newaxis] - 2 * np.dot(X,\n",
        "                                                                                                               self.X_train.T))\n",
        "        pass\n",
        "        return dists\n",
        "\n",
        "    def predict_labels(self, dists, k=1):\n",
        "        num_test = dists.shape[0]\n",
        "        y_pred = np.zeros(num_test)\n",
        "        for i in range(num_test):\n",
        "            closest_y = []\n",
        "            sorted_dist = np.argsort(dists[i])\n",
        "            closest_y = list(self.y_train[sorted_dist[0:k]])\n",
        "            pass\n",
        "            y_pred[i] = (np.argmax(np.bincount(closest_y)))\n",
        "            pass\n",
        "        return y_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "goEpIEMxkTYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = Figlet(font='digital')\n",
        "print(f.renderText('Question 7: Classification (KNN)'))\n",
        "\n",
        "cifar10_dir = './cifar-10-batches-py'\n",
        "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "\n",
        "# Checking the size of the training and testing data\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "metadata": {
        "id": "nlkTFY6vklsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory error prevention by subsampling data\n",
        "\n",
        "num_training = 10000\n",
        "mask = list(range(num_training))\n",
        "X_train = X_train[mask]\n",
        "y_train = y_train[mask]\n",
        "\n",
        "num_test = 1000\n",
        "mask = list(range(num_test))\n",
        "X_test = X_test[mask]\n",
        "y_test = y_test[mask]\n",
        "\n",
        "# reshaping data and placing into rows\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "print(X_train.shape, X_test.shape)\n",
        "\n",
        "classifier = KNearestNeighbor()\n",
        "classifier.train(X_train, y_train)\n",
        "dists = classifier.compute_distances(X_test)\n",
        "y_test_pred = classifier.predict_labels(dists, k=5)\n",
        "num_correct = np.sum(y_test_pred == y_test)\n",
        "accuracy = float(num_correct) / num_test\n",
        "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))\n",
        "\n",
        "num_folds = 5\n",
        "k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n",
        "\n",
        "X_train_folds = []\n",
        "y_train_folds = []\n",
        "\n",
        "X_train_folds = np.array_split(X_train, num_folds)\n",
        "y_train_folds = np.array_split(y_train, num_folds)\n",
        "k_to_accuracies = {}\n",
        "\n",
        "for k in k_choices:\n",
        "    k_to_accuracies[k] = []\n",
        "    for num_knn in range(0, num_folds):\n",
        "        X_test = X_train_folds[num_knn]\n",
        "        y_test = y_train_folds[num_knn]\n",
        "        X_train = X_train_folds\n",
        "        y_train = y_train_folds\n",
        "\n",
        "        temp = np.delete(X_train, num_knn, 0)\n",
        "        X_train = np.concatenate((temp), axis=0)\n",
        "        y_train = np.delete(y_train, num_knn, 0)\n",
        "        y_train = np.concatenate((y_train), axis=0)\n",
        "\n",
        "        classifier = KNearestNeighbor()\n",
        "        classifier.train(X_train, y_train)\n",
        "        dists = classifier.compute_distances(X_test)\n",
        "        y_test_pred = classifier.predict_labels(dists, k)\n",
        "\n",
        "        num_correct = np.sum(y_test_pred == y_test)\n",
        "        accuracy = float(num_correct) / num_test\n",
        "        #         print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))\n",
        "        k_to_accuracies[k].append(accuracy)\n",
        "\n",
        "print(\"Printing our 5-fold accuracies for varying values of k:\")\n",
        "print()\n",
        "for k in sorted(k_to_accuracies):\n",
        "    for accuracy in k_to_accuracies[k]:\n",
        "        print('k = %d, accuracy = %f' % (k, accuracy))"
      ],
      "metadata": {
        "id": "bn1eusEEk6qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree Classifier**"
      ],
      "metadata": {
        "id": "pBi4Jp_ylYTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f.renderText('Question 7: Classification (Decision Tree Classifier)'))"
      ],
      "metadata": {
        "id": "2GIT1uD6a7hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "94NUFE7pbhR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DTC(_X=None, _Xt=None):\n",
        "    if _X is None:\n",
        "        _X = X_train\n",
        "\n",
        "    if _Xt is None:\n",
        "        _Xt = X_test\n",
        "\n",
        "    print(\"[DTC] Training\")\n",
        "    dtc = DecisionTreeClassifier()\n",
        "    dtc.fit(X_train, y_train)\n",
        "\n",
        "    print(\"[DTC] Training Accuracy\")\n",
        "    X_pred = dtc.predict(X_train)\n",
        "    print(metrics.accuracy_score(y_train, X_pred))\n",
        "\n",
        "    print(\"[DTC] Testing Accuracy\")\n",
        "    Xt_pred = dtc.predict(X_test)\n",
        "    print(metrics.accuracy_score(y_test, Xt_pred))\n",
        "\n",
        "DTC()"
      ],
      "metadata": {
        "id": "NaMxDdPEbjQ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}