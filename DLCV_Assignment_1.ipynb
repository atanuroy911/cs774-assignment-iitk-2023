{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf4c42299f994f8c9efcfeaf598e8945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b00d2cef3141467f8e6a6dec31b5e366",
              "IPY_MODEL_f7bb4acfa1f2429088687a9e3ec22d88",
              "IPY_MODEL_bbc552ed99734e05a6804d65af003c79"
            ],
            "layout": "IPY_MODEL_b93169d2d0ca420c97bfdda1e8c88a62"
          }
        },
        "b00d2cef3141467f8e6a6dec31b5e366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e5ae03f8e04e8593a3c1f48782c566",
            "placeholder": "​",
            "style": "IPY_MODEL_b2407c4f28aa458d9e913628de6303ef",
            "value": "100%"
          }
        },
        "f7bb4acfa1f2429088687a9e3ec22d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e86a9db0704842a6972bc77f4b028114",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60be88b9e6c94ee2a0f7bca96164a430",
            "value": 46830571
          }
        },
        "bbc552ed99734e05a6804d65af003c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d48eb78658d242579bc97ab77b087803",
            "placeholder": "​",
            "style": "IPY_MODEL_bb702134f5bf4c73b27c5b570929458e",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 77.0MB/s]"
          }
        },
        "b93169d2d0ca420c97bfdda1e8c88a62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27e5ae03f8e04e8593a3c1f48782c566": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2407c4f28aa458d9e913628de6303ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e86a9db0704842a6972bc77f4b028114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60be88b9e6c94ee2a0f7bca96164a430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d48eb78658d242579bc97ab77b087803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb702134f5bf4c73b27c5b570929458e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Extra Libraries**"
      ],
      "metadata": {
        "id": "hmZzY7RGLXig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyfiglet\n",
        "%pip install tqdm"
      ],
      "metadata": {
        "id": "ivyubcb3n00G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3bebcce-8438-4c59-c456-18c08b23e006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyfiglet\n",
            "  Downloading pyfiglet-0.8.post1-py2.py3-none-any.whl (865 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.8/865.8 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyfiglet\n",
            "Successfully installed pyfiglet-0.8.post1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "KFqt6N_TLgOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from os.path import exists\n",
        "import numpy as np\n",
        "import sys\n",
        "import math\n",
        "import cv2\n",
        "import urllib\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "\n",
        "\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import use as mpl_use\n",
        "from pyfiglet import Figlet\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "print('Import Complete')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tjJXDEILfia",
        "outputId": "468923b8-f91f-4e43-dcca-dd24cc169ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Extractor**"
      ],
      "metadata": {
        "id": "KX6DQYxQPPXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_name_to_module(model):\n",
        "    name_to_module = {}\n",
        "    for m in model.named_modules():\n",
        "        name_to_module[m[0]] = m[1]\n",
        "    return name_to_module\n",
        "\n",
        "\n",
        "def get_activation(all_outputs, name):\n",
        "    def hook(model, input, output):\n",
        "        all_outputs[name] = output.detach()\n",
        "\n",
        "    return hook\n",
        "\n",
        "\n",
        "def add_hooks(model, outputs, output_layer_names):\n",
        "    \"\"\"\n",
        "    :param model:\n",
        "    :param outputs: Outputs from layers specified in `output_layer_names` will be stored in `output` variable\n",
        "    :param output_layer_names:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    name_to_module = get_name_to_module(model)\n",
        "    for output_layer_name in output_layer_names:\n",
        "        name_to_module[output_layer_name].register_forward_hook(get_activation(outputs, output_layer_name))\n",
        "\n",
        "\n",
        "class ModelWrapper(nn.Module):\n",
        "    def __init__(self, model, output_layer_names, return_single=True):\n",
        "        super(ModelWrapper, self).__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.output_layer_names = output_layer_names\n",
        "        self.outputs = {}\n",
        "        self.return_single = return_single\n",
        "        add_hooks(self.model, self.outputs, self.output_layer_names)\n",
        "\n",
        "    def forward(self, images):\n",
        "        self.model(images)\n",
        "        output_vals = [self.outputs[output_layer_name] for output_layer_name in self.output_layer_names]\n",
        "        if self.return_single:\n",
        "            return output_vals[0]\n",
        "        else:\n",
        "            return output_vals\n",
        "\n",
        "\n",
        "class BBResNet18(object):\n",
        "    def __init__(self):\n",
        "        self.model = resnet18(pretrained=True)\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.eval()\n",
        "\n",
        "        self.model = ModelWrapper(self.model, ['avgpool'], True)\n",
        "\n",
        "        self.model.eval()\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def feature_extraction(self, x: np.ndarray):\n",
        "        '''\n",
        "            param:\n",
        "                x: numpy ndarray of shape: [None, 3, 224, 224] and dtype: np.float32\n",
        "            \n",
        "            return:\n",
        "                numpy ndarray (feature vector) of shape: [None, 512] and dtype: np.float32\n",
        "        '''\n",
        "\n",
        "        x = torch.from_numpy(x).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = self.model(x).cpu().detach()\n",
        "            out = out.view(out.size(0), -1)\n",
        "            out = out.numpy()\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "CeK7iwgaPSlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utility Fuctions**"
      ],
      "metadata": {
        "id": "x1IM9HhcPZXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset():\n",
        "    print('Downloading CIFAR Dataset')\n",
        "    import tarfile\n",
        "    print('Download Complete. Extracting ...')\n",
        "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    file = tarfile.open(fileobj=response.raw, mode=\"r|gz\")\n",
        "    file.extractall(path=\".\")\n",
        "    print('Extraction Completed')\n",
        "\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "\n",
        "def download_url(url, output_path):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
        "\n",
        "# Function for checking the internet connectivity\n",
        "\n",
        "def connect(host='http://google.com'):\n",
        "    try:\n",
        "        urllib.request.urlopen(host)  # Python 3.x\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def url_exists(path):\n",
        "    r = requests.head(path)\n",
        "    return r.status_code == requests.codes.ok\n",
        "\n",
        "# Unpack the dataset using pickle library\n",
        "def unpickle(folder):\n",
        "    labels_mapping = {}\n",
        "    # np.set_printoptions(threshold=sys.maxsize)\n",
        "    train_data = {'data': np.array([]), 'labels': []}\n",
        "    test_data = {'data': np.array([]), 'labels': []}\n",
        "    print(os.listdir(folder))\n",
        "    for file in os.listdir(folder):\n",
        "        if file == \"data_batch_1\" or file == \"data_batch_2\" or file == \"data_batch_3\" or file == \"data_batch_4\" or file == \"data_batch_5\":\n",
        "            print('Currently Processing File : ', file)\n",
        "            dict = unpickle_each(os.path.join(folder, file))\n",
        "            if train_data['data'].shape[0] == 0:\n",
        "                train_data['data'] = dict[b'data']\n",
        "            else:\n",
        "                train_data['data'] = np.vstack([train_data['data'], dict[b'data']])\n",
        "            train_data['labels'] = train_data['labels'] + dict[b'labels']\n",
        "        elif file == 'batches.meta':\n",
        "            print('Currently Processing File : ', file)\n",
        "            labels_mapping = unpickle_each(os.path.join(folder, file))\n",
        "        elif file == 'test_batch':\n",
        "            print('Currently Processing File : ', file)\n",
        "            dict = unpickle_each(os.path.join(folder, file))\n",
        "            if test_data['data'].shape[0] == 0:\n",
        "                test_data['data'] = dict[b'data']\n",
        "            test_data['labels'] = test_data['labels'] + dict[b'labels']\n",
        "    return train_data, test_data, labels_mapping\n",
        "    # with open(file, 'rb') as fo:\n",
        "    #     dict = pickle.load(fo, encoding='bytes')\n",
        "    # return dict\n",
        "\n",
        "def unpickle_each(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "\n",
        "def preprocessing(data):\n",
        "    images = []\n",
        "    count = 1\n",
        "    for img in data['data']:\n",
        "        img_new = img.reshape((3, 32, 32))\n",
        "        # print(img_new[0][0][0])\n",
        "        image = np.transpose(img_new, [1, 2, 0])\n",
        "        images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "\n",
        "def img_enhancement(img):\n",
        "    # print(img)\n",
        "    arr_pxl = []\n",
        "    for i in range(img.shape[0]):\n",
        "        for j in range(img.shape[1]):\n",
        "            avg = np.average(img[i, j])\n",
        "            arr_pxl.append(avg)\n",
        "    imax = np.max(arr_pxl)\n",
        "    imin = np.min(arr_pxl)\n",
        "\n",
        "    for i in range(img.shape[0]):\n",
        "        for j in range(img.shape[1]):\n",
        "            pxl = []\n",
        "            avg = np.average(img[i, j])\n",
        "            ip = 255 * (avg - imin) / (imax - imin)\n",
        "            for c in range(img.shape[2]):\n",
        "                if img[i, j, c] == 0:\n",
        "                    n_pxl = 0\n",
        "                else:\n",
        "                    n_pxl = int((img[i, j, c] * ip / avg))\n",
        "                pxl.append(n_pxl)\n",
        "            img[i, j, :] = np.array(pxl)\n",
        "            # print(img.shape)\n",
        "    return img, img.shape\n",
        "\n",
        "\n",
        "def img_posterization(img):\n",
        "    imin = random.randint(30, 50)\n",
        "    imax = random.randint(100, 150)\n",
        "    range_pxl = abs(imax - imin)\n",
        "    divider = 255 / range_pxl\n",
        "\n",
        "    for i in range(img.shape[0]):\n",
        "        for j in range(img.shape[1]):\n",
        "            pxl = []\n",
        "            for c in range(img.shape[2]):\n",
        "                cont_pxl = (img[i, j, c] / divider)\n",
        "                cont_pxl = (cont_pxl + imin)\n",
        "                pxl.append(cont_pxl)\n",
        "            img[i, j, :] = np.array(pxl).clip(0, 255)\n",
        "    return img, img.shape\n",
        "\n",
        "\n",
        "# This function rotates the image around its center by random degree between [-180, 180].\n",
        "def random_rotation(image):\n",
        "    # Choose Random degree\n",
        "    degree = random.randint(-180, 180)\n",
        "    # print(\"Random degree chosen: \", degree)\n",
        "    # First we will convert the degrees into radians\n",
        "    rads = math.radians(degree)\n",
        "    cosine = math.cos(rads)\n",
        "    sine = math.sin(rads)\n",
        "\n",
        "    # Find the height and width of the rotated image using cosine and sine transformations\n",
        "    height_rot_img = round(abs(image.shape[0] * cosine)) + round(abs(image.shape[1] * sine))\n",
        "    width_rot_img = round(abs(image.shape[1] * cosine)) + round(abs(image.shape[0] * sine))\n",
        "\n",
        "    # Initialising the rotated image by zeros\n",
        "    rot_img = np.uint8(np.zeros((height_rot_img, width_rot_img, image.shape[2])))\n",
        "\n",
        "    # Finding the center point of the original image\n",
        "    orgx, orgy = (image.shape[1] // 2, image.shape[0] // 2)\n",
        "\n",
        "    # Finding the center point of rotated image.\n",
        "    rotx, roty = (width_rot_img // 2, height_rot_img // 2)\n",
        "\n",
        "    for i in range(rot_img.shape[0]):\n",
        "        for j in range(rot_img.shape[1]):\n",
        "            # Find the all new coordinates for orginal image wrt the new center point\n",
        "            x = (i - rotx) * cosine + (j - roty) * sine\n",
        "            y = -(i - rotx) * sine + (j - roty) * cosine\n",
        "\n",
        "            x = round(x) + orgy\n",
        "            y = round(y) + orgx\n",
        "\n",
        "            # Restricting the index in between original height and width of image.\n",
        "            if x >= 0 and y >= 0 and x < image.shape[0] and y < image.shape[1]:\n",
        "                rot_img[i, j, :] = image[x, y, :]\n",
        "    return rot_img, degree\n",
        "\n",
        "\n",
        "def contrast_and_flip(image):\n",
        "    img_height = image.shape[0]\n",
        "    img_width = image.shape[1]\n",
        "    # Adding two pixels padding across the image\n",
        "    img = np.uint8(np.zeros((img_height, img_width, image.shape[2])))\n",
        "\n",
        "    alpha = random.uniform(0.5, 2.0)\n",
        "    flip_prob = random.randint(0, 1)\n",
        "    # print(\"Alpha value: \", alpha)\n",
        "\n",
        "    for i in range(img.shape[0]):\n",
        "        for j in range(img.shape[1]):\n",
        "            pxl = []\n",
        "            for c in range(image.shape[2]):\n",
        "                cont_pxl = int(alpha * (image[i, j, c] - 128) + 128)\n",
        "                if cont_pxl > 255:\n",
        "                    cont_pxl = 255\n",
        "                elif cont_pxl < 0:\n",
        "                    cont_pxl = 0\n",
        "                pxl.append(cont_pxl)\n",
        "            img[i, j, :] = np.array(pxl)\n",
        "    if (flip_prob):\n",
        "        # print(\"Including Horizontal Flipping\")\n",
        "        img = img[:, ::-1, :]  # Horizontal Flipping\n",
        "    return img, round(alpha, 3)\n",
        "\n",
        "\n",
        "# Generating Augmented Images\n",
        "def get_augmented_images(data, labels):\n",
        "    augmented_img = []\n",
        "    augmented_labels = []\n",
        "    preprocess_func = {0: random_rotation, 1: img_enhancement, 2: img_posterization, 3: contrast_and_flip}\n",
        "    i = 0\n",
        "    # print(data)\n",
        "    for img in data:\n",
        "        rndm_idx = random.randint(0, 3)\n",
        "        if i % 1000 == 0:\n",
        "            print(\"\\nProcessing Image Number: \", i, end=' ')\n",
        "        # Resizing to restore rotated image's dimensions to 32 x 32\n",
        "        if preprocess_func[rndm_idx] != random_rotation:\n",
        "            n_img, _ = preprocess_func[rndm_idx](img)\n",
        "        else:\n",
        "            # print(img)\n",
        "            n_img, _ = preprocess_func[rndm_idx](img)\n",
        "            n_img = cv2.resize(n_img, (32, 32))\n",
        "        augmented_img.append(n_img)\n",
        "        augmented_labels.append(labels[i])\n",
        "        i += 1\n",
        "    return np.array(augmented_img), augmented_labels\n",
        "\n",
        "\n",
        "def get_feat_vec(images, obj):\n",
        "    feat_vec = []\n",
        "    count = 1\n",
        "    for img in images:\n",
        "        # print(count)\n",
        "        img = cv2.resize(img, (224, 224))\n",
        "        img = np.transpose(img, (2, 1, 0))\n",
        "        # Performing Normalization before sending into ResNet model\n",
        "        img = img / 255\n",
        "        img = np.array(img, dtype=np.float32)\n",
        "        feat_vec.append(obj.feature_extraction(np.array([img]))[0])\n",
        "        count += 1\n",
        "    return np.array(feat_vec)\n"
      ],
      "metadata": {
        "id": "plEDVWHTPcUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download CIFAR-10 Dataset**"
      ],
      "metadata": {
        "id": "nOfvr0XUP8C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = Figlet(font='slant')\n",
        "print(f.renderText('CS774 Assignment 1'))\n",
        "\n",
        "f = Figlet(font='digital')\n",
        "print(f.renderText('by Anjali Manoj and Atanu Shuvam Roy'))\n",
        "\n",
        "# Check for internet\n",
        "print(\n",
        "    \"Connected to Internet. Ready for duty.\" if connect() else \"No Internet! Put Extracted Dataset in current directory\")\n",
        "\n",
        "dir = 'cifar-10-batches-py'\n",
        "file_exists = exists('cifar-10-python.tar.gz')\n",
        "dir_exists = exists(dir)\n",
        "if file_exists:\n",
        "    print('Previous File Exists. Removing File ...')\n",
        "    os.unlink('cifar-10-python.tar.gz')\n",
        "elif dir_exists:\n",
        "    files_required = ['batches.meta', 'data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5',\n",
        "                      'test_batch']\n",
        "    flag = True\n",
        "    for i in files_required:\n",
        "        path = Path(dir + f'/{i}')\n",
        "        if not path.is_file():\n",
        "            flag = False\n",
        "            break\n",
        "    if flag:\n",
        "        print('Dataset already available')\n",
        "    else:\n",
        "        print('Dataset Incomplete. Re-download required')\n",
        "        shutil.rmtree(dir)\n",
        "else:\n",
        "    print('Dataset Unavailable. Connecting to internet ...')\n",
        "    download_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMACdunkQEvd",
        "outputId": "24ef20f7-6718-47c3-e179-2451d7972e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   _______________________ __\n",
            "  / ____/ ___/__  /__  / // /\n",
            " / /    \\__ \\  / /  / / // /_\n",
            "/ /___ ___/ / / /  / /__  __/\n",
            "\\____//____/ /_/  /_/  /_/   \n",
            "                             \n",
            "    ___              _                                  __     ___\n",
            "   /   |  __________(_)___ _____  ____ ___  ___  ____  / /_   <  /\n",
            "  / /| | / ___/ ___/ / __ `/ __ \\/ __ `__ \\/ _ \\/ __ \\/ __/   / / \n",
            " / ___ |(__  |__  ) / /_/ / / / / / / / / /  __/ / / / /_    / /  \n",
            "/_/  |_/____/____/_/\\__, /_/ /_/_/ /_/ /_/\\___/_/ /_/\\__/   /_/   \n",
            "                   /____/                                         \n",
            "\n",
            "+-+-+ +-+-+-+-+-+-+ +-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+ +-+-+-+-+-+-+ +-+-+-+\n",
            "|b|y| |A|n|j|a|l|i| |M|a|n|o|j| |a|n|d| |A|t|a|n|u| |S|h|u|v|a|m| |R|o|y|\n",
            "+-+-+ +-+-+-+-+-+-+ +-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+ +-+-+-+-+-+-+ +-+-+-+\n",
            "\n",
            "Connected to Internet. Ready for duty.\n",
            "Dataset Unavailable. Connecting to internet ...\n",
            "Downloading CIFAR Dataset\n",
            "Download Complete. Extracting ...\n",
            "Extraction Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load The Dataset**"
      ],
      "metadata": {
        "id": "ISIBnDfnQPki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = Figlet(font='digital')\n",
        "print(f.renderText('Question 1: Loading the dataset'))\n",
        "\n",
        "train_data, test_data, labels_mapping = unpickle(dir)\n",
        "\n",
        "# np.set_printoptions(threshold=sys.maxsize)\n",
        "# print(train_data['data'][0])\n",
        "\n",
        "print(\"Total train data size:\", train_data['data'].shape)\n",
        "print(\"Total test data size:\", test_data['data'].shape)\n",
        "print(\"Labels available for CIFAR-10: \", labels_mapping[b'label_names'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIu8ScLwQSA8",
        "outputId": "3eaa838e-9a6b-4fec-be21-25ecd1c452d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |1|:| |L|o|a|d|i|n|g| |t|h|e| |d|a|t|a|s|e|t|\n",
            "+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+-+-+\n",
            "\n",
            "['data_batch_1', 'test_batch', 'data_batch_5', 'data_batch_2', 'batches.meta', 'readme.html', 'data_batch_3', 'data_batch_4']\n",
            "Currently Processing File :  data_batch_1\n",
            "Currently Processing File :  test_batch\n",
            "Currently Processing File :  data_batch_5\n",
            "Currently Processing File :  data_batch_2\n",
            "Currently Processing File :  batches.meta\n",
            "Currently Processing File :  data_batch_3\n",
            "Currently Processing File :  data_batch_4\n",
            "Total train data size: (50000, 3072)\n",
            "Total test data size: (10000, 3072)\n",
            "Labels available for CIFAR-10:  [b'airplane', b'automobile', b'bird', b'cat', b'deer', b'dog', b'frog', b'horse', b'ship', b'truck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Image Processing/Augmentation**"
      ],
      "metadata": {
        "id": "1UXSvUQ2QYYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2: Image transformations\n",
        "\n",
        "f = Figlet(font='digital')\n",
        "print(f.renderText('Question 2 (a): Image Enhancement'))\n",
        "\n",
        "org_train_images = preprocessing(train_data)\n",
        "org_test_images = preprocessing(test_data)\n",
        "\n",
        "# Test Image for applying image enhancement\n",
        "first_image = org_train_images[0]\n",
        "\n",
        "# Matplotlib Backend Specify to ignore the bug (MacOSX)\n",
        "# mpl_use('MacOSX')\n",
        "plt.figure(figsize=(2, 2))\n",
        "# plt.imshow(first_image)\n",
        "# plt.show()\n",
        "\n",
        "# Question 2 (a)\n",
        "plt.figure(figsize=(2, 2))\n",
        "first_image = org_train_images[random.randint(0, 49000)]\n",
        "enhanced_image, _ = img_enhancement(first_image)\n",
        "plt.imshow(enhanced_image)\n",
        "plt.show()\n",
        "\n",
        "# Question 2 (b)\n",
        "print(f.renderText('Question 2 (b): Posterization of Image'))\n",
        "first_image = org_train_images[random.randint(0, 49000)]\n",
        "posterized_image, _ = img_posterization(first_image)\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(posterized_image)\n",
        "plt.show()\n",
        "\n",
        "# Question 2 (c)\n",
        "print(f.renderText('Question 2 (c): Random Rotate'))\n",
        "first_image = org_train_images[random.randint(0, 49000)]\n",
        "rotated_image, rotated_degree = random_rotation(first_image)\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(rotated_image)\n",
        "plt.show()\n",
        "\n",
        "print(f.renderText('Question 2 (d): Contrast and Horizontal Flipping'))\n",
        "first_image = org_train_images[random.randint(0, 49000)]\n",
        "contrast_image, alpha = contrast_and_flip(first_image)\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(contrast_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "uvpnlpt0QjF2",
        "outputId": "76bd07c1-1287-4bd8-9055-a81d99f69782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+ +-+-+-+-+-+-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |2| |(|a|)|:| |I|m|a|g|e| |E|n|h|a|n|c|e|m|e|n|t|\n",
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+ +-+-+-+-+-+-+-+-+-+-+-+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 144x144 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |2| |(|b|)|:| |P|o|s|t|e|r|i|z|a|t|i|o|n| |o|f| |I|m|a|g|e|\n",
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+-+ +-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |2| |(|c|)|:| |R|a|n|d|o|m| |R|o|t|a|t|e|\n",
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+-+ +-+-+-+-+-+-+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |2| |(|d|)|:| |C|o|n|t|r|a|s|t| |a|n|d| |H|o|r|i|z|o|n|t|a|l|\n",
            "+-+-+-+-+-+-+-+-+ +-+ +-+-+-+-+ +-+-+-+-+-+-+-+-+ +-+-+-+ +-+-+-+-+-+-+-+-+-+-+\n",
            "+-+-+-+-+-+-+-+-+\n",
            "|F|l|i|p|p|i|n|g|\n",
            "+-+-+-+-+-+-+-+-+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Augmented Image Set**"
      ],
      "metadata": {
        "id": "pCQcnm2-R7yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f.renderText('Question 3: Augmented Images'))\n",
        "\n",
        "# Generating Augmented Images\n",
        "cifar_augmented_dir = './cifar-10-batches-augmented-py'\n",
        "# Check whether the specified path exists or not\n",
        "isExist = os.path.exists(cifar_augmented_dir)\n",
        "if not isExist:\n",
        "    # Create a new directory because it does not exist\n",
        "    os.makedirs(cifar_augmented_dir)\n",
        "    print(\"The new directory (cifar-10-batches-augmented-py) is created!\")\n",
        "\n",
        "print('Checking for preprocessed augmented data')\n",
        "if os.path.exists(cifar_augmented_dir + '/augmented_batch.npy') and os.path.exists(cifar_augmented_dir + '/augmented_batch_labels.npy'):\n",
        "    print('Preprocessed augmented data found. Loading Data')\n",
        "    augmented_train_set = np.load(cifar_augmented_dir + '/augmented_batch.npy')\n",
        "    augmented_train_labels = np.load(cifar_augmented_dir + '/augmented_batch_labels.npy')\n",
        "else:\n",
        "    print('Preprocessed augmented data NOT found. Checking Online ...')\n",
        "    aug_batch_url =\"https://cse.iitk.ac.in/users/atanusroy/augmented_batch.npy\";\n",
        "    aug_batch_labels_url =\"https://cse.iitk.ac.in/users/atanusroy/augmented_batch_labels.npy\";\n",
        "    if(url_exists(aug_batch_url) and url_exists(aug_batch_labels_url)):\n",
        "      download_url(aug_batch_url, cifar_augmented_dir + '/augmented_batch.npy')\n",
        "      download_url(aug_batch_labels_url, cifar_augmented_dir + '/augmented_batch_labels.npy')\n",
        "      augmented_train_set = np.load(cifar_augmented_dir + '/augmented_batch.npy')\n",
        "      augmented_train_labels = np.load(cifar_augmented_dir + '/augmented_batch_labels.npy')\n",
        "    else:\n",
        "      print('File Not Available Online. Regenerating Augmented Dataset ...')\n",
        "      train_augmented_img, train_augmented_labels = get_augmented_images(org_train_images, train_data['labels'])\n",
        "      augmented_train_set = np.vstack([org_train_images, train_augmented_img])\n",
        "      augmented_train_labels = train_data['labels'] + train_augmented_labels\n",
        "      np.save(cifar_augmented_dir + '/augmented_batch.npy', augmented_train_set)\n",
        "      np.save(cifar_augmented_dir + '/augmented_batch_labels.npy', augmented_train_labels)\n",
        "      print('Augmented Data Shape: ', train_augmented_img.shape)\n",
        "    print(\"Original Data Shape: \", org_train_images.shape)\n",
        "print('Size of New Training Data Set: ', len(augmented_train_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkgSjTc6SDl-",
        "outputId": "22a29b4f-ee9b-46b4-c643-66662c8cc3c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+-+-+-+-+ +-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |3|:| |A|u|g|m|e|n|t|e|d| |I|m|a|g|e|s|\n",
            "+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+-+-+-+-+ +-+-+-+-+-+-+\n",
            "\n",
            "The new directory (cifar-10-batches-augmented-py) is created!\n",
            "Checking for preprocessed augmented data\n",
            "Preprocessed augmented data NOT found. Checking Online ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "augmented_batch.npy: 307MB [20:03, 255kB/s]                           \n",
            "augmented_batch_labels.npy: 803kB [00:05, 151kB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Shape:  (50000, 32, 32, 3)\n",
            "Size of New Training Data Set:  100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get Feature Vectors**"
      ],
      "metadata": {
        "id": "pGQmZMO9U55A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f.renderText('Question 4: Feature Vector'))\n",
        "\n",
        "obj = BBResNet18()\n",
        "\n",
        "feature_vec_dir = './feature_vectors'\n",
        "# Check whether the specified path exists or not\n",
        "isExist = os.path.exists(feature_vec_dir)\n",
        "if not isExist:\n",
        "    # Create a new directory because it does not exist\n",
        "    os.makedirs(feature_vec_dir)\n",
        "    print(\"The new directory (feature_vec_dir) is created!\")\n",
        "\n",
        "if exists('./feature_vectors/original_train_feature_vector.npy') and exists(\n",
        "        './feature_vectors/original_test_feature_vector.npy'):\n",
        "    print(\"Loading Original Data Training Feature Vector\")\n",
        "\n",
        "    original_train_feat_vec = np.load('./feature_vectors/original_train_feature_vector.npy')\n",
        "    original_test_feat_vec = np.load('./feature_vectors/original_test_feature_vector.npy')\n",
        "else:\n",
        "    print(\"Generating Original Data Training Feature Vector\")\n",
        "\n",
        "    original_train_feat_vec = get_feat_vec(org_train_images, obj)\n",
        "    original_test_feat_vec = get_feat_vec(org_test_images, obj)\n",
        "\n",
        "    np.save('./feature_vectors/original_train_feature_vector.npy', original_train_feat_vec)\n",
        "    np.save('./feature_vectors/original_test_feature_vector.npy', original_test_feat_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624,
          "referenced_widgets": [
            "cf4c42299f994f8c9efcfeaf598e8945",
            "b00d2cef3141467f8e6a6dec31b5e366",
            "f7bb4acfa1f2429088687a9e3ec22d88",
            "bbc552ed99734e05a6804d65af003c79",
            "b93169d2d0ca420c97bfdda1e8c88a62",
            "27e5ae03f8e04e8593a3c1f48782c566",
            "b2407c4f28aa458d9e913628de6303ef",
            "e86a9db0704842a6972bc77f4b028114",
            "60be88b9e6c94ee2a0f7bca96164a430",
            "d48eb78658d242579bc97ab77b087803",
            "bb702134f5bf4c73b27c5b570929458e"
          ]
        },
        "id": "T80pYhWUU8y5",
        "outputId": "45ab62ad-8c0c-43b8-f0cc-5a1fea61174d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+-+-+ +-+-+-+-+-+-+\n",
            "|Q|u|e|s|t|i|o|n| |4|:| |F|e|a|t|u|r|e| |V|e|c|t|o|r|\n",
            "+-+-+-+-+-+-+-+-+ +-+-+ +-+-+-+-+-+-+-+ +-+-+-+-+-+-+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf4c42299f994f8c9efcfeaf598e8945"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new directory (feature_vec_dir) is created!\n",
            "Generating Original Data Training Feature Vector\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-df1af8c05e88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating Original Data Training Feature Vector\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0moriginal_train_feat_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feat_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_train_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0moriginal_test_feat_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feat_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_test_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5d1617060a2f>\u001b[0m in \u001b[0;36mget_feat_vec\u001b[0;34m(images, obj)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mfeat_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ec1c5eb05a04>\u001b[0m in \u001b[0;36mfeature_extraction\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ec1c5eb05a04>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moutput_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_layer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput_layer_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_single\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MLP IMPLEMENTATION**"
      ],
      "metadata": {
        "id": "FEOGTc1VVOar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
        "\n",
        "def relu(x, deriv=False):\n",
        "    # ReLU activation function\n",
        "    if (deriv):\n",
        "        return np.where(x <= 0, 0, 1)\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    '''\n",
        "        softmax(x) = exp(x) / sum(exp(x))\n",
        "    '''\n",
        "\n",
        "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    prob = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "    return prob\n",
        "\n",
        "\n",
        "def cross_entropy_loss(predicted, target):\n",
        "    \"\"\"\n",
        "    predicted (batch_size x num_classes)\n",
        "    target is labels (batch_size x 1)\n",
        "    \"\"\"\n",
        "    log_likelihood = -np.log(predicted[range(target.shape[0]), target])\n",
        "    loss = -np.sum(log_likelihood) / target.shape[0]\n",
        "    # print(\"Loss: \", np.sum(loss))\n",
        "    # exit()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def get_one_hot_vector(y):\n",
        "    \"\"\"\n",
        "    generating one hot vector for class labels\n",
        "    \"\"\"\n",
        "    y = np.array(y)\n",
        "    y_one_hot = np.zeros((y.shape[0], 10))\n",
        "    y_one_hot[np.arange(y.shape[0]), y] = 1\n",
        "    # y_one_hot = y_one_hot.reshape(-1, 10, 1)\n",
        "    return y_one_hot\n",
        "\n",
        "\n",
        "class MLP(object):\n",
        "    def __init__(self, input_size, load_model_weights, augmented):\n",
        "        if load_model_weights:\n",
        "            print(\"Loading from pretrained model\")\n",
        "            if (augmented):\n",
        "                read_weights = np.load(load_model_weights + '/augmented-model_weights.npy', allow_pickle='TRUE').item()\n",
        "            else:\n",
        "                read_weights = np.load(load_model_weights + '/unaugmented-model_weights.npy',\n",
        "                                       allow_pickle='TRUE').item()\n",
        "            self.weights = read_weights['weights']\n",
        "            self.biases = read_weights['biases']\n",
        "        else:\n",
        "            print(\"Initialising weights and biases\")\n",
        "\n",
        "            self.weights = [np.random.randn(y, x) * 0.1 for x, y in\n",
        "                            zip(input_size[:-2], input_size[1:-1])] + [\n",
        "                               np.random.randn(input_size[-1], input_size[-2]) * 0.1]\n",
        "            self.biases = [np.zeros((x, 1)) for x in input_size[1:-1]] + [np.zeros((input_size[-1], 1))]\n",
        "\n",
        "        print(\"Weights Shape for Input Layer: \", self.weights[0].shape)\n",
        "        print(\"Weights Shape for Hidden Layer 1: \", self.weights[1].shape)\n",
        "        print(\"Weights Shape for Hidden Layer 2: \", self.weights[2].shape)\n",
        "\n",
        "        print(\"Bias Shape for Input Layer: \", self.biases[0].shape)\n",
        "        print(\"Bias Shape for Hidden Layer 1: \", self.biases[1].shape)\n",
        "        print(\"Bias Shape for Hidden Layer 2: \", self.biases[2].shape)\n",
        "\n",
        "    def feedforward(self, x):\n",
        "\n",
        "        z1 = np.dot(self.weights[0], x) + self.biases[0][:, :x.shape[1]]\n",
        "        a1 = relu(z1)\n",
        "        z2 = np.dot(self.weights[1], a1) + self.biases[1][:, :a1.shape[1]]\n",
        "        a2 = relu(z2)\n",
        "        z3 = np.dot(self.weights[2], a2) + self.biases[2][:, :a2.shape[1]]\n",
        "        a3 = softmax(z3.T)\n",
        "\n",
        "        return z1, a1, z2, a2, z3, a3\n",
        "\n",
        "    def backpropagation(self, X, y):\n",
        "\n",
        "        delta_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        delta_b = [np.zeros(b.shape) for b in self.biases]\n",
        "\n",
        "        z1, a1, z2, a2, z3, a3 = self.feedforward(X)\n",
        "        y = y.argmax(axis=1)\n",
        "        loss = cross_entropy_loss(a3, y)\n",
        "        a3[range(y.shape[0]), y] -= 1\n",
        "        error = a3\n",
        "\n",
        "        # For Output Layer\n",
        "        delta1 = error.T\n",
        "        delta_b[2] = delta1\n",
        "        delta_w[2] = np.dot(delta1, a2.T)\n",
        "\n",
        "        # For Second Hidden Layer\n",
        "        deriv_relu = relu(z2, deriv=True)\n",
        "        delta2 = np.dot(self.weights[2].T, delta1) * deriv_relu\n",
        "        delta_b[1] = delta2\n",
        "        delta_w[1] = np.dot(delta2, a1.T)\n",
        "\n",
        "        # For First Hidden Layer\n",
        "        deriv_relu = relu(z1, deriv=True)\n",
        "        delta3 = np.dot(self.weights[1].T, delta2) * deriv_relu\n",
        "        delta_b[0] = delta3\n",
        "        delta_w[0] = np.dot(delta3, X.T)\n",
        "\n",
        "        return loss, delta_b, delta_w\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "\n",
        "        count = 0\n",
        "        preds = np.array([])\n",
        "        for x, _y in zip(X, y):\n",
        "            _, _, _, _, _, a3 = self.feedforward(np.array([x]).T)\n",
        "            preds = np.append(preds, np.argmax(a3))\n",
        "            if np.argmax(a3) == np.argmax(_y):\n",
        "                count += 1\n",
        "        return float(count) / X.shape[0], preds\n",
        "\n",
        "    def train(self, X, y, X_test, y_test, learning_rate=0.01, epochs=5, batch_size=100):\n",
        "        history_training_loss, history_training_acc, history_test_acc, history_test_pred = [], [], [], []\n",
        "        model_weights = {}\n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle\n",
        "            permutation = np.random.permutation(X.shape[0])\n",
        "            x_train_shuffled = X[permutation]\n",
        "            y_train_shuffled = y[permutation]\n",
        "\n",
        "            # same shape as self.biases\n",
        "            del_b = [np.zeros(b.shape) for b in self.biases]\n",
        "            # same shape as self.weights\n",
        "            del_w = [np.zeros(w.shape) for w in self.weights]\n",
        "            flag = 0\n",
        "            loss_min = 9999\n",
        "\n",
        "            for batch_idx in range(0, X.shape[0], batch_size):\n",
        "                # Selecting batches of the training images\n",
        "                batch_x = x_train_shuffled[batch_idx:batch_idx + batch_size]\n",
        "                batch_y = y_train_shuffled[batch_idx:batch_idx + batch_size]\n",
        "\n",
        "                # Performing Backpropagation\n",
        "                loss, delta_b, delta_w = self.backpropagation(batch_x.T, batch_y)\n",
        "                loss = abs(loss)\n",
        "                # Maintaining the original shape according to batch size when input is less than given batch size\n",
        "                if delta_b[0].shape[1] < batch_size:\n",
        "                    # print(delta_b[0].shape) \n",
        "                    size_left = batch_size - delta_b[0].shape[1]\n",
        "                    delta_b[0] = np.pad(delta_b[0], ((0, 0), (0, size_left)))\n",
        "\n",
        "                if delta_b[1].shape[1] < batch_size:\n",
        "                    # print(delta_b[1].shape)\n",
        "                    size_left = batch_size - delta_b[1].shape[1]\n",
        "                    delta_b[1] = np.pad(delta_b[1], ((0, 0), (0, size_left)))\n",
        "\n",
        "                if delta_b[2].shape[1] < batch_size:\n",
        "                    # print(delta_b[2].shape)\n",
        "                    size_left = batch_size - delta_b[2].shape[1]\n",
        "                    delta_b[2] = np.pad(delta_b[2], ((0, 0), (0, size_left)))\n",
        "\n",
        "                #     #It will encounter only when there's minimum loss is getting as compared to previous batches\n",
        "                if loss < loss_min:\n",
        "                    loss_min = loss\n",
        "                    del_b = [db + ddb for db, ddb in zip(del_b, delta_b)]\n",
        "                    del_w = [dw + ddw for dw, ddw in zip(del_w, delta_w)]\n",
        "\n",
        "            self.weights = [w - (learning_rate / batch_size) * dw for w, dw in zip(self.weights, del_w)]\n",
        "            self.biases = [b - (learning_rate / batch_size) * db for b, db in zip(self.biases, del_b)]\n",
        "\n",
        "            model_weights['weights'] = self.weights\n",
        "            model_weights['biases'] = self.biases\n",
        "\n",
        "            # Evaluate performance\n",
        "            train_acc, _ = self.evaluate(X, y)\n",
        "            test_acc, test_pred = self.evaluate(X_test, y_test)\n",
        "            history_training_loss.append(loss_min)\n",
        "            history_training_acc.append(train_acc)\n",
        "            history_test_acc.append(test_acc)\n",
        "            history_test_pred.append(test_pred)\n",
        "            print(\"Epoch: %d Training loss: %.3f Training accuracy: %.2f Testing Accuracy: %.2f\" % (\n",
        "                epoch, loss_min, train_acc * 100, test_acc * 100))\n",
        "        return np.array(history_training_acc), np.array(history_training_loss), np.array(history_test_acc), np.array(\n",
        "            history_test_pred), model_weights\n",
        "\n",
        "\n",
        "def Model(X_train, y_train, X_test, y_test, model_wt_folder, out_folder, isModelWeightsAvailable=0, epochs=500,\n",
        "          batch_size=32, learning_rate=0.01, augmented=False):\n",
        "    inp_feats = 512\n",
        "    num_neurons_1 = 64\n",
        "    num_neurons_2 = 64\n",
        "    num_output = 10\n",
        "    # batch_size=256\n",
        "    # learning_rate=0.01\n",
        "    print(\"Epochs: \", epochs)\n",
        "    print(\"y_train: \", y_train.shape)\n",
        "    print(\"batch-size: \", batch_size)\n",
        "    print(\"learning_rate: \", learning_rate)\n",
        "    if isModelWeightsAvailable:\n",
        "        print(\"Performance measure on test datasets\")\n",
        "        model = MLP((inp_feats, num_neurons_1, num_neurons_2, num_output), model_wt_folder, augmented)\n",
        "        acc, prediction = model.evaluate(X_test, y_test)\n",
        "        print(\"Testing Accuracy or Performance Measure in percent: %.2f\" % (acc * 100))\n",
        "    else:\n",
        "        model = MLP((inp_feats, num_neurons_1, num_neurons_2, num_output), '', augmented)\n",
        "        total_training_acc, total_training_loss, total_testing_acc, total_test_pred, model_weights = model.train(\n",
        "            X_train, y_train, X_test, y_test, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)\n",
        "        # print(model_weights['weights'].shape)\n",
        "        if (augmented):\n",
        "            np.save(model_wt_folder + '/augmented-model_weights.npy', model_weights)\n",
        "        else:\n",
        "            np.save(model_wt_folder + '/unaugmented-model_weights.npy', model_weights)\n",
        "        # acc = model.evaluate(X_test, y_test)\n",
        "        print(\"Testing Accuracy or Performance Measure in percent: %.2f at epoch: %d\" % (\n",
        "            total_testing_acc[np.argmax(total_training_acc)] * 100, np.argmax(total_training_acc) + 1))\n",
        "        print(\"With a loss: %.3f\" % (total_training_loss[np.argmax(total_training_acc)]))\n",
        "        prediction = total_test_pred[np.argmax(total_training_acc)]\n",
        "\n",
        "        output_dir = './output'\n",
        "        # Check whether the specified path exists or not\n",
        "        isExist = os.path.exists(output_dir)\n",
        "        if not isExist:\n",
        "            # Create a new directory because it does not exist\n",
        "            os.makedirs(output_dir)\n",
        "            print(\"The new directory (output) is created!\")\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.plot(np.arange(epochs), total_training_acc, label='Total_trainig_acc')\n",
        "        plt.plot(np.arange(epochs), total_training_loss, label='Total_training_loss')\n",
        "        plt.plot(np.arange(epochs), total_testing_acc, label='Total_testing_acc')\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Training Accuracy and Loss\")\n",
        "        plt.title(\n",
        "            \"Loss/Accuracy vs Epochs - \" + \"batch_size-\" + str(batch_size) + \"-learning-rate-\" + str(learning_rate))\n",
        "        plt.legend()\n",
        "        if (augmented):\n",
        "            plt.savefig(out_folder + \"/augmented-loss-accuracy-graph-\" + str(batch_size) + \"-\" + str(\n",
        "                learning_rate) + \".jpg\")\n",
        "        else:\n",
        "            plt.savefig(\n",
        "                out_folder + \"/unaugmented-loss-accuracy-graph-\" + str(batch_size) + \"-\" + str(learning_rate) + \".jpg\")\n",
        "\n",
        "    # Y_test = y_test.argmax(axis=1)\n",
        "    # print('Accuracy: %.3f' % (accuracy_score(Y_test, prediction)*100))\n",
        "    # print('Precision: %.3f' % precision_score(Y_test, prediction, average='weighted'))\n",
        "    # print('Recall: %.3f'% recall_score(Y_test, prediction, average='weighted'))\n",
        "    # print('F1 score: %.3f'% f1_score(Y_test, prediction, average='weighted'))\n",
        "    # print('confussion matrix:\\n',confusion_matrix(Y_test, prediction))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ISJOtwGWV5c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Call MLP**"
      ],
      "metadata": {
        "id": "6fNyGALlViHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f.renderText('Question 5 & 6(a): MLP Implementation'))\n",
        "\n",
        "model_weights_dir = './model_weights'\n",
        "# Check whether the specified path exists or not\n",
        "isExist = os.path.exists(model_weights_dir)\n",
        "if not isExist:\n",
        "    # Create a new directory because it does not exist\n",
        "    os.makedirs(model_weights_dir)\n",
        "    print(\"The new directory (model_weights) is created!\")\n",
        "\n",
        "output_dir = './output'\n",
        "# Check whether the specified path exists or not\n",
        "isExist = os.path.exists(output_dir)\n",
        "if not isExist:\n",
        "    # Create a new directory because it does not exist\n",
        "    os.makedirs(output_dir)\n",
        "    print(\"The new directory (output) is created!\")\n",
        "\n",
        "labels = np.arange(10)\n",
        "print(\"Training on Augmented Datasets\")\n",
        "\n",
        "model_dir = './models'\n",
        "# Check whether the specified path exists or not\n",
        "isExist = os.path.exists(model_dir)\n",
        "if not isExist:\n",
        "    # Create a new directory because it does not exist\n",
        "    os.makedirs(model_dir)\n",
        "    print(\"The new directory (models) is created!\")\n",
        "print(\"Training on Un-Augmented Datasets\")\n",
        "train_labels = get_one_hot_vector(train_data['labels'])\n",
        "test_labels = get_one_hot_vector(test_data['labels'])\n",
        "unaugmented_model = Model(original_train_feat_vec, train_labels, original_test_feat_vec, test_labels, './model_weights',\n",
        "                          './output', isModelWeightsAvailable=0, epochs=5, batch_size=128, learning_rate=0.01,\n",
        "                          augmented=False)\n",
        "torch.save(unaugmented_model, './models/unaugmented_model')\n",
        "print(f.renderText('Question 6 (b): Back Propagation'))\n",
        "\n",
        "print(\"Checking Augmented Data Training Feature Vector\")\n",
        "\n",
        "if exists('./feature_vectors/augmented_train_feature_vector.npy'):\n",
        "    print(\"Loading Augmented Data Training Feature Vector\")\n",
        "\n",
        "    augmented_train_feat_vec = np.load('./feature_vectors/original_train_feature_vector.npy')\n",
        "else:\n",
        "    print(\"Generating Augmented Data Training Feature Vector\")\n",
        "    augmented_train_feat_vec = get_feat_vec(augmented_train_set, obj)\n",
        "    np.save('./feature_vectors/augmented_train_feature_vector.npy', augmented_train_feat_vec)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "aug_train_labels = get_one_hot_vector(augmented_train_labels)\n",
        "augmented_model = Model(augmented_train_feat_vec, aug_train_labels, original_test_feat_vec, test_labels,\n",
        "                        './model_weights',\n",
        "                        './output', isModelWeightsAvailable=0, epochs=5, batch_size=128, learning_rate=0.01,\n",
        "                        augmented=True)\n",
        "torch.save(augmented_model, './models/augmented_model')"
      ],
      "metadata": {
        "id": "G0nLlv3vVlJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Load for Classification**\n",
        "Must Run for Classification"
      ],
      "metadata": {
        "id": "r51C-wYokuPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import platform\n",
        "\n",
        "def load_pickle(f):\n",
        "    version = platform.python_version_tuple()\n",
        "    if version[0] == '2':\n",
        "        return  pickle.load(f)\n",
        "    elif version[0] == '3':\n",
        "        return  pickle.load(f, encoding='latin1')\n",
        "    raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = load_pickle(f)\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "  \"\"\" load all of cifar \"\"\"\n",
        "  xs = []\n",
        "  ys = []\n",
        "  for b in range(1,6):\n",
        "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "    X, Y = load_CIFAR_batch(f)\n",
        "    xs.append(X)\n",
        "    ys.append(Y)\n",
        "  Xtr = np.concatenate(xs)\n",
        "  Ytr = np.concatenate(ys)\n",
        "  del X, Y\n",
        "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "  return Xtr, Ytr, Xte, Yte"
      ],
      "metadata": {
        "id": "arQKbyBzkydC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **KNN Classification**"
      ],
      "metadata": {
        "id": "8bwRBnwpkQhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification\n",
        "# KNearest Neighbor Classification\n",
        "\n",
        "\n",
        "class KNearestNeighbor(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def train(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X, k=1, num_loops=0):\n",
        "        if num_loops == 0:\n",
        "            dists = self.compute_distances(X)\n",
        "        else:\n",
        "            raise ValueError('Invalid value %d for num_loops' % num_loops)\n",
        "        return self.predict_labels(dists, k=k)\n",
        "\n",
        "    def compute_distances(self, X):\n",
        "        num_test = X.shape[0]\n",
        "        num_train = self.X_train.shape[0]\n",
        "        dists = np.zeros((num_test, num_train))\n",
        "        dists = np.sqrt(\n",
        "            np.sum(np.square(self.X_train), axis=1) + np.sum(np.square(X), axis=1)[:, np.newaxis] - 2 * np.dot(X,\n",
        "                                                                                                               self.X_train.T))\n",
        "        pass\n",
        "        return dists\n",
        "\n",
        "    def predict_labels(self, dists, k=1):\n",
        "        num_test = dists.shape[0]\n",
        "        y_pred = np.zeros(num_test)\n",
        "        for i in range(num_test):\n",
        "            closest_y = []\n",
        "            sorted_dist = np.argsort(dists[i])\n",
        "            closest_y = list(self.y_train[sorted_dist[0:k]])\n",
        "            pass\n",
        "            y_pred[i] = (np.argmax(np.bincount(closest_y)))\n",
        "            pass\n",
        "        return y_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "goEpIEMxkTYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = Figlet(font='digital')\n",
        "print(f.renderText('Question 7: Classification (KNN)'))\n",
        "\n",
        "cifar10_dir = './cifar-10-batches-py'\n",
        "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "\n",
        "# Checking the size of the training and testing data\n",
        "print('Training data shape: ', X_train.shape)\n",
        "print('Training labels shape: ', y_train.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "metadata": {
        "id": "nlkTFY6vklsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory error prevention by subsampling data\n",
        "\n",
        "num_training = 10000\n",
        "mask = list(range(num_training))\n",
        "X_train = X_train[mask]\n",
        "y_train = y_train[mask]\n",
        "\n",
        "num_test = 1000\n",
        "mask = list(range(num_test))\n",
        "X_test = X_test[mask]\n",
        "y_test = y_test[mask]\n",
        "\n",
        "# reshaping data and placing into rows\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "print(X_train.shape, X_test.shape)\n",
        "\n",
        "classifier = KNearestNeighbor()\n",
        "classifier.train(X_train, y_train)\n",
        "dists = classifier.compute_distances(X_test)\n",
        "y_test_pred = classifier.predict_labels(dists, k=5)\n",
        "num_correct = np.sum(y_test_pred == y_test)\n",
        "accuracy = float(num_correct) / num_test\n",
        "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))\n",
        "\n",
        "num_folds = 5\n",
        "k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n",
        "\n",
        "X_train_folds = []\n",
        "y_train_folds = []\n",
        "\n",
        "X_train_folds = np.array_split(X_train, num_folds)\n",
        "y_train_folds = np.array_split(y_train, num_folds)\n",
        "k_to_accuracies = {}\n",
        "\n",
        "for k in k_choices:\n",
        "    k_to_accuracies[k] = []\n",
        "    for num_knn in range(0, num_folds):\n",
        "        X_test = X_train_folds[num_knn]\n",
        "        y_test = y_train_folds[num_knn]\n",
        "        X_train = X_train_folds\n",
        "        y_train = y_train_folds\n",
        "\n",
        "        temp = np.delete(X_train, num_knn, 0)\n",
        "        X_train = np.concatenate((temp), axis=0)\n",
        "        y_train = np.delete(y_train, num_knn, 0)\n",
        "        y_train = np.concatenate((y_train), axis=0)\n",
        "\n",
        "        classifier = KNearestNeighbor()\n",
        "        classifier.train(X_train, y_train)\n",
        "        dists = classifier.compute_distances(X_test)\n",
        "        y_test_pred = classifier.predict_labels(dists, k)\n",
        "\n",
        "        num_correct = np.sum(y_test_pred == y_test)\n",
        "        accuracy = float(num_correct) / num_test\n",
        "        #         print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))\n",
        "        k_to_accuracies[k].append(accuracy)\n",
        "\n",
        "print(\"Printing our 5-fold accuracies for varying values of k:\")\n",
        "print()\n",
        "for k in sorted(k_to_accuracies):\n",
        "    for accuracy in k_to_accuracies[k]:\n",
        "        print('k = %d, accuracy = %f' % (k, accuracy))"
      ],
      "metadata": {
        "id": "bn1eusEEk6qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree Classifier**"
      ],
      "metadata": {
        "id": "pBi4Jp_ylYTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f.renderText('Question 7: Classification (Decision Tree Classifier)'))"
      ],
      "metadata": {
        "id": "2GIT1uD6a7hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "94NUFE7pbhR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DTC(_X=None, _Xt=None):\n",
        "    if _X is None:\n",
        "        _X = X_train\n",
        "\n",
        "    if _Xt is None:\n",
        "        _Xt = X_test\n",
        "\n",
        "    print(\"[DTC] Training\")\n",
        "    dtc = DecisionTreeClassifier()\n",
        "    dtc.fit(X_train, y_train)\n",
        "\n",
        "    print(\"[DTC] Training Accuracy\")\n",
        "    X_pred = dtc.predict(X_train)\n",
        "    print(metrics.accuracy_score(y_train, X_pred))\n",
        "\n",
        "    print(\"[DTC] Testing Accuracy\")\n",
        "    Xt_pred = dtc.predict(X_test)\n",
        "    print(metrics.accuracy_score(y_test, Xt_pred))\n",
        "\n",
        "DTC()"
      ],
      "metadata": {
        "id": "NaMxDdPEbjQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression**"
      ],
      "metadata": {
        "id": "Sh0HRiY2Xspi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Things required to unpack the CIFAR-10 library\n",
        "import os\n",
        "# import h5py\n",
        "import six\n",
        "from six.moves import range, cPickle\n",
        "import tarfile\n",
        "\n",
        "# Main Library for Matrices manipulation\n",
        "import numpy as np\n",
        "\n",
        "# To draw the images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "\n",
        "\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "\n",
        "def download_url(url, output_path):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
        "\n",
        "\n",
        "# download_url(url, './cifar-10-python.tar.gz')\n",
        "\n",
        "\n",
        "def pydump(obj, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "\n",
        "def pyload(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "def cifar_10():\n",
        "    # LOAD TRAINING DATA\n",
        "    tar_file = tarfile.open(\"cifar-10-python.tar.gz\", 'r:gz')\n",
        "    train_batches = []\n",
        "    for batch in range(1, 6):\n",
        "        file = tar_file.extractfile(\n",
        "            'cifar-10-batches-py/data_batch_%d' % batch)\n",
        "        try:\n",
        "            if six.PY3:\n",
        "                array = cPickle.load(file, encoding='latin1')\n",
        "            else:\n",
        "                array = cPickle.load(file)\n",
        "            train_batches.append(array)\n",
        "        finally:\n",
        "            file.close()\n",
        "\n",
        "    train_features = np.concatenate(\n",
        "        [batch['data'].reshape(batch['data'].shape[0], 3, 32, 32)\n",
        "         for batch in train_batches])\n",
        "    train_labels = np.concatenate(\n",
        "        [np.array(batch['labels'], dtype=np.uint8)\n",
        "         for batch in train_batches])\n",
        "    train_labels = np.expand_dims(train_labels, 1)\n",
        "\n",
        "    # LOAD TEST DATA\n",
        "    file = tar_file.extractfile('cifar-10-batches-py/test_batch')\n",
        "    try:\n",
        "        if six.PY3:\n",
        "            test = cPickle.load(file, encoding='latin1')\n",
        "        else:\n",
        "            test = cPickle.load(file)\n",
        "    finally:\n",
        "        file.close()\n",
        "\n",
        "    test_features = test['data'].reshape(test['data'].shape[0],\n",
        "                                         3, 32, 32)\n",
        "    test_labels = np.array(test['labels'], dtype=np.uint8)\n",
        "    test_labels = np.expand_dims(test_labels, 1)\n",
        "\n",
        "    return train_features, train_labels, test_features, test_labels\n",
        "\n",
        "\n",
        "train_features, train_labels, test_features, test_labels = cifar_10()\n",
        "X = train_features.reshape(50000, 3 * 32 * 32)\n",
        "Xt = test_features.reshape(10000, 3 * 32 * 32)\n",
        "y = train_labels.flatten()\n",
        "yt = test_labels.flatten()\n",
        "\n",
        "linreg = LogisticRegression(verbose=True)\n",
        "linreg.fit(X, y)\n",
        "\n",
        "predicted = linreg.predict(X)\n",
        "np.unique((y == 0).astype(np.int8))\n",
        "\n",
        "predicted_r = np.round(predicted)\n",
        "print(metrics.accuracy_score(y, predicted))\n",
        "\n",
        "test_predicted = linreg.predict(Xt)\n",
        "print(metrics.accuracy_score(yt, test_predicted))"
      ],
      "metadata": {
        "id": "NIRB43efXv3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM Classifier**"
      ],
      "metadata": {
        "id": "70xjfN_2crFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.svm as svm\n",
        "\n",
        "\n",
        "def SVM_SVC(itr=1, _X=None, _Xt=None):\n",
        "    if _X is None:\n",
        "        _X = X\n",
        "\n",
        "    if _Xt is None:\n",
        "        _Xt = Xt\n",
        "\n",
        "    print(\"[SVM POLY %d] Training\" % itr)\n",
        "    svc = svm.SVC(max_iter=itr, kernel='poly')\n",
        "    svc.fit(X, y)\n",
        "\n",
        "    print(\"[SVM POLY %d] Training Accuracy\" % itr)\n",
        "    X_pred = svc.predict(X)\n",
        "    print(metrics.accuracy_score(y, X_pred))\n",
        "\n",
        "    print(\"[SVM POLY %d] Testing Accuracy\" % itr)\n",
        "    Xt_pred = svc.predict(Xt)\n",
        "    print(metrics.accuracy_score(yt, Xt_pred))\n",
        "\n",
        "\n",
        "def SVM_SVC_SIG(_X=None, _Xt=None, I=2):\n",
        "    if _X is None:\n",
        "        _X = X\n",
        "\n",
        "    if _Xt is None:\n",
        "        _Xt = Xt\n",
        "\n",
        "    print(\"[SVM SIG %d] Training\" % I)\n",
        "    svc = svm.SVC(kernel='sigmoid', max_iter=I)\n",
        "    svc.fit(X, y)\n",
        "\n",
        "    print(\"[SVM SIG %d] Training Accuracy\" % I)\n",
        "    X_pred = svc.predict(X)\n",
        "    print(metrics.accuracy_score(y, X_pred))\n",
        "\n",
        "    print(\"[SVM SIG %d] Testing Accuracy\" % I)\n",
        "    Xt_pred = svc.predict(Xt)\n",
        "    print(metrics.accuracy_score(yt, Xt_pred))\n",
        "\n",
        "\n",
        "for i in [500, 1000, 2000, 3000, -1]:\n",
        "    SVM_SVC_SIG(I=i)"
      ],
      "metadata": {
        "id": "zZXRxOfLcuqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}